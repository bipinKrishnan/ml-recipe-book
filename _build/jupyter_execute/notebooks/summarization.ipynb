{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-03-12T07:19:33.79052Z",
     "iopub.status.busy": "2022-03-12T07:19:33.789303Z",
     "iopub.status.idle": "2022-03-12T07:19:47.454608Z",
     "shell.execute_reply": "2022-03-12T07:19:47.453385Z",
     "shell.execute_reply.started": "2022-03-12T07:19:33.790365Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q datasets accelerate wandb rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T07:19:47.457638Z",
     "iopub.status.busy": "2022-03-12T07:19:47.457267Z",
     "iopub.status.idle": "2022-03-12T07:19:57.412489Z",
     "shell.execute_reply": "2022-03-12T07:19:57.411374Z",
     "shell.execute_reply.started": "2022-03-12T07:19:47.457595Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, DatasetDict, concatenate_datasets, load_metric\n",
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from accelerate import Accelerator\n",
    "import nltk\n",
    "import wandb\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T07:19:57.415329Z",
     "iopub.status.busy": "2022-03-12T07:19:57.415017Z",
     "iopub.status.idle": "2022-03-12T07:21:18.605691Z",
     "shell.execute_reply": "2022-03-12T07:21:18.604608Z",
     "shell.execute_reply.started": "2022-03-12T07:19:57.415285Z"
    }
   },
   "outputs": [],
   "source": [
    "english_dataset = load_dataset(\"amazon_reviews_multi\", \"en\")\n",
    "french_dataset = load_dataset(\"amazon_reviews_multi\", \"fr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T07:21:18.609345Z",
     "iopub.status.busy": "2022-03-12T07:21:18.60861Z",
     "iopub.status.idle": "2022-03-12T07:21:18.616653Z",
     "shell.execute_reply": "2022-03-12T07:21:18.615604Z",
     "shell.execute_reply.started": "2022-03-12T07:21:18.609295Z"
    }
   },
   "outputs": [],
   "source": [
    "english_dataset.set_format('pandas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T07:21:18.619185Z",
     "iopub.status.busy": "2022-03-12T07:21:18.618229Z",
     "iopub.status.idle": "2022-03-12T07:21:19.053248Z",
     "shell.execute_reply": "2022-03-12T07:21:19.052301Z",
     "shell.execute_reply.started": "2022-03-12T07:21:18.619137Z"
    }
   },
   "outputs": [],
   "source": [
    "english_dataset['train'][:]['product_category'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T07:21:19.056528Z",
     "iopub.status.busy": "2022-03-12T07:21:19.055822Z",
     "iopub.status.idle": "2022-03-12T07:21:19.062852Z",
     "shell.execute_reply": "2022-03-12T07:21:19.061309Z",
     "shell.execute_reply.started": "2022-03-12T07:21:19.05648Z"
    }
   },
   "outputs": [],
   "source": [
    "english_dataset.reset_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T07:21:19.064837Z",
     "iopub.status.busy": "2022-03-12T07:21:19.064503Z",
     "iopub.status.idle": "2022-03-12T07:21:30.662208Z",
     "shell.execute_reply": "2022-03-12T07:21:30.661299Z",
     "shell.execute_reply.started": "2022-03-12T07:21:19.064789Z"
    }
   },
   "outputs": [],
   "source": [
    "def filter_reviews(examples):\n",
    "    return examples['product_category']=='kitchen'\n",
    "\n",
    "english_dataset = english_dataset.filter(filter_reviews)\n",
    "french_dataset = french_dataset.filter(filter_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T07:21:30.664842Z",
     "iopub.status.busy": "2022-03-12T07:21:30.664074Z",
     "iopub.status.idle": "2022-03-12T07:21:30.68539Z",
     "shell.execute_reply": "2022-03-12T07:21:30.684383Z",
     "shell.execute_reply.started": "2022-03-12T07:21:30.664762Z"
    }
   },
   "outputs": [],
   "source": [
    "english_dataset['train'].shuffle(seed=42)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T07:21:30.689827Z",
     "iopub.status.busy": "2022-03-12T07:21:30.689003Z",
     "iopub.status.idle": "2022-03-12T07:21:40.534068Z",
     "shell.execute_reply": "2022-03-12T07:21:40.533013Z",
     "shell.execute_reply.started": "2022-03-12T07:21:30.689758Z"
    }
   },
   "outputs": [],
   "source": [
    "checkpoint = \"google/mt5-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T07:21:40.53865Z",
     "iopub.status.busy": "2022-03-12T07:21:40.53776Z",
     "iopub.status.idle": "2022-03-12T07:21:40.578185Z",
     "shell.execute_reply": "2022-03-12T07:21:40.577257Z",
     "shell.execute_reply.started": "2022-03-12T07:21:40.538605Z"
    }
   },
   "outputs": [],
   "source": [
    "final_dataset = DatasetDict()\n",
    "\n",
    "for split in english_dataset.keys():\n",
    "    final_dataset[split] = concatenate_datasets([english_dataset[split], french_dataset[split]])\n",
    "    final_dataset[split] = final_dataset[split].shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T07:21:40.580371Z",
     "iopub.status.busy": "2022-03-12T07:21:40.57981Z",
     "iopub.status.idle": "2022-03-12T07:21:40.591441Z",
     "shell.execute_reply": "2022-03-12T07:21:40.590255Z",
     "shell.execute_reply.started": "2022-03-12T07:21:40.580327Z"
    }
   },
   "outputs": [],
   "source": [
    "final_dataset['train'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T07:21:40.594177Z",
     "iopub.status.busy": "2022-03-12T07:21:40.592997Z",
     "iopub.status.idle": "2022-03-12T07:21:41.66963Z",
     "shell.execute_reply": "2022-03-12T07:21:41.668484Z",
     "shell.execute_reply.started": "2022-03-12T07:21:40.594115Z"
    }
   },
   "outputs": [],
   "source": [
    "final_dataset = final_dataset.filter(lambda x: len(x['review_title']) > 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T07:21:41.67181Z",
     "iopub.status.busy": "2022-03-12T07:21:41.671393Z",
     "iopub.status.idle": "2022-03-12T07:21:41.67957Z",
     "shell.execute_reply": "2022-03-12T07:21:41.678612Z",
     "shell.execute_reply.started": "2022-03-12T07:21:41.671764Z"
    }
   },
   "outputs": [],
   "source": [
    "max_input_length = 512\n",
    "max_output_length = 30\n",
    "\n",
    "def tokenize(examples):\n",
    "    inputs = tokenizer(examples['review_body'], truncation=True, max_length=max_input_length)\n",
    "    \n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples['review_title'], truncation=True, max_length=max_output_length)\n",
    "        \n",
    "    inputs['labels'] = labels['input_ids']\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T07:21:41.681964Z",
     "iopub.status.busy": "2022-03-12T07:21:41.681296Z",
     "iopub.status.idle": "2022-03-12T07:21:46.294351Z",
     "shell.execute_reply": "2022-03-12T07:21:46.293288Z",
     "shell.execute_reply.started": "2022-03-12T07:21:41.681919Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = final_dataset.map(\n",
    "    tokenize, \n",
    "    batched=True, \n",
    "    remove_columns=final_dataset['train'].column_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T07:21:46.296634Z",
     "iopub.status.busy": "2022-03-12T07:21:46.29597Z",
     "iopub.status.idle": "2022-03-12T07:22:36.337227Z",
     "shell.execute_reply": "2022-03-12T07:22:36.336297Z",
     "shell.execute_reply.started": "2022-03-12T07:21:46.296583Z"
    }
   },
   "outputs": [],
   "source": [
    "bs = 8\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "collate_fn = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    tokenized_datasets['train'], \n",
    "    batch_size=bs, \n",
    "    shuffle=False, collate_fn=collate_fn\n",
    ")\n",
    "val_dl = DataLoader(\n",
    "    tokenized_datasets['validation'], \n",
    "    batch_size=bs, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "test_dl = DataLoader(\n",
    "    tokenized_datasets['test'], \n",
    "    batch_size=bs, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T07:22:36.339233Z",
     "iopub.status.busy": "2022-03-12T07:22:36.338904Z",
     "iopub.status.idle": "2022-03-12T07:22:43.0679Z",
     "shell.execute_reply": "2022-03-12T07:22:43.066759Z",
     "shell.execute_reply.started": "2022-03-12T07:22:36.339176Z"
    }
   },
   "outputs": [],
   "source": [
    "opt = torch.optim.AdamW(model.parameters(), lr=1e-2)\n",
    "metric = load_metric('rouge')\n",
    "\n",
    "accelerator = Accelerator()\n",
    "train_dl, val_dl, test_dl, model, opt = accelerator.prepare(\n",
    "    train_dl, val_dl, test_dl, model, opt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T07:22:43.070126Z",
     "iopub.status.busy": "2022-03-12T07:22:43.069642Z",
     "iopub.status.idle": "2022-03-12T07:22:43.385607Z",
     "shell.execute_reply": "2022-03-12T07:22:43.384493Z",
     "shell.execute_reply.started": "2022-03-12T07:22:43.070081Z"
    }
   },
   "outputs": [],
   "source": [
    "generated_summary = \"I absolutely loved reading the Hunger Games.\\n I absolutely loved reading the Hunger Games\"\n",
    "reference_summary = \"I loved reading the Hunger Games.\\n I loved reading the Hunger Games\"\n",
    "\n",
    "scores = metric.compute(\n",
    "    predictions=[generated_summary], references=[reference_summary]\n",
    ")\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T07:22:43.387961Z",
     "iopub.status.busy": "2022-03-12T07:22:43.387413Z",
     "iopub.status.idle": "2022-03-12T07:22:43.397377Z",
     "shell.execute_reply": "2022-03-12T07:22:43.396105Z",
     "shell.execute_reply.started": "2022-03-12T07:22:43.387912Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_preds_and_labels(preds, labels):\n",
    "    preds = preds.detach().cpu()\n",
    "    labels = labels.detach().cpu()\n",
    "    # replace all -100 with the token id of <pad>\n",
    "    labels = torch.where(labels==-100, tokenizer.pad_token_id, labels)\n",
    "    \n",
    "    # decode all token ids to its string/text format\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    return decoded_preds, decoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T07:22:43.399522Z",
     "iopub.status.busy": "2022-03-12T07:22:43.399131Z",
     "iopub.status.idle": "2022-03-12T07:22:43.412115Z",
     "shell.execute_reply": "2022-03-12T07:22:43.41114Z",
     "shell.execute_reply.started": "2022-03-12T07:22:43.399453Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_training(train_dl):\n",
    "    model.train()\n",
    "    for batch in tqdm(train_dl, total=len(train_dl)):\n",
    "        opt.zero_grad()\n",
    "        out = model(**batch)\n",
    "        accelerator.backward(out.loss)\n",
    "        opt.step()\n",
    "        \n",
    "def run_evaluation(test_dl):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dl, total=len(test_dl)):\n",
    "            # generate predictions one by one\n",
    "            preds = model.generate(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                max_length=max_output_length,\n",
    "            )\n",
    "            \n",
    "            # convert target labels and predictions to string format for computing ROUGE score\n",
    "            preds, labels = process_preds_and_labels(preds, batch['labels'])\n",
    "            # add the target labels and predictions of this batch to metrics\n",
    "            metric.add_batch(predictions=preds, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T07:22:43.414529Z",
     "iopub.status.busy": "2022-03-12T07:22:43.413872Z",
     "iopub.status.idle": "2022-03-12T07:22:43.429894Z",
     "shell.execute_reply": "2022-03-12T07:22:43.428804Z",
     "shell.execute_reply.started": "2022-03-12T07:22:43.414482Z"
    }
   },
   "outputs": [],
   "source": [
    "def save_model(epoch):\n",
    "    model_path = f\"model-v{epoch+1}.pt\"\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    artifact = wandb.Artifact(\"model\", type=\"model\")\n",
    "    artifact.add_file(model_path)\n",
    "    run.log_artifact(artifact)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T07:22:43.432171Z",
     "iopub.status.busy": "2022-03-12T07:22:43.431835Z",
     "iopub.status.idle": "2022-03-12T08:21:27.975204Z",
     "shell.execute_reply": "2022-03-12T08:21:27.973445Z",
     "shell.execute_reply.started": "2022-03-12T07:22:43.432125Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "run = wandb.init(\n",
    "    project=\"summarization\", \n",
    "    group='training', \n",
    "    entity=\"bipin\", \n",
    "    name=\"run-lr-1e-2-rerun\", \n",
    "    reinit=True\n",
    ")\n",
    "\n",
    "with run:\n",
    "    for epoch in range(epochs):\n",
    "        run_training(train_dl)\n",
    "        \n",
    "        run_evaluation(test_dl)\n",
    "        # calculate ROUGE score on test set\n",
    "        test_acc = metric.compute()\n",
    "        print(f\"epoch: {epoch} test_acc: {test_acc}\")\n",
    "    \n",
    "        save_model(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LRFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T06:41:30.027672Z",
     "iopub.status.busy": "2022-03-12T06:41:30.026758Z",
     "iopub.status.idle": "2022-03-12T06:41:30.033557Z",
     "shell.execute_reply": "2022-03-12T06:41:30.032282Z",
     "shell.execute_reply.started": "2022-03-12T06:41:30.027639Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm.autonotebook import tqdm\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T06:41:30.082024Z",
     "iopub.status.busy": "2022-03-12T06:41:30.081363Z",
     "iopub.status.idle": "2022-03-12T06:41:30.185823Z",
     "shell.execute_reply": "2022-03-12T06:41:30.184751Z",
     "shell.execute_reply.started": "2022-03-12T06:41:30.081979Z"
    }
   },
   "outputs": [],
   "source": [
    "class LRFinder(object):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        device=None,\n",
    "    ):\n",
    "        # Check if the optimizer is already attached to a scheduler\n",
    "        self.optimizer = optimizer\n",
    "        self._check_for_scheduler()\n",
    "\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.history = {\"lr\": [], \"loss\": []}\n",
    "        self.best_loss = None\n",
    "\n",
    "        # If device is None, use the same as the model\n",
    "        self.accelerator = Accelerator()\n",
    "        self.device = self.accelerator.device\n",
    "\n",
    "    def range_test(\n",
    "        self,\n",
    "        train_iter,\n",
    "        val_iter=None,\n",
    "        start_lr=None,\n",
    "        end_lr=10,\n",
    "        num_iter=100,\n",
    "        step_mode=\"exp\",\n",
    "        smooth_f=0.05,\n",
    "        diverge_th=5,\n",
    "        accumulation_steps=1,\n",
    "        non_blocking_transfer=True,\n",
    "    ):\n",
    "        # Reset test results\n",
    "        self.history = {\"lr\": [], \"loss\": []}\n",
    "        self.best_loss = None\n",
    "\n",
    "        # Move the model to the proper device\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Check if the optimizer is already attached to a scheduler\n",
    "        self._check_for_scheduler()\n",
    "\n",
    "        # Set the starting learning rate\n",
    "        if start_lr:\n",
    "            self._set_learning_rate(start_lr)\n",
    "\n",
    "        # Initialize the proper learning rate policy\n",
    "        if step_mode.lower() == \"exp\":\n",
    "            lr_schedule = ExponentialLR(self.optimizer, end_lr, num_iter)\n",
    "        elif step_mode.lower() == \"linear\":\n",
    "            lr_schedule = LinearLR(self.optimizer, end_lr, num_iter)\n",
    "        else:\n",
    "            raise ValueError(\"expected one of (exp, linear), got {}\".format(step_mode))\n",
    "\n",
    "        if smooth_f < 0 or smooth_f >= 1:\n",
    "            raise ValueError(\"smooth_f is outside the range [0, 1[\")\n",
    "\n",
    "        for iteration in tqdm(range(num_iter)):\n",
    "            # Train on batch and retrieve loss\n",
    "            loss = self._train_batch(\n",
    "                train_iter,\n",
    "                accumulation_steps,\n",
    "                non_blocking_transfer=non_blocking_transfer,\n",
    "            )\n",
    "            if val_iter:\n",
    "                loss = self._validate(\n",
    "                    val_iter, non_blocking_transfer=non_blocking_transfer\n",
    "                )\n",
    "\n",
    "            # Update the learning rate\n",
    "            self.history[\"lr\"].append(lr_schedule.get_lr()[0])\n",
    "            lr_schedule.step()\n",
    "\n",
    "            # Track the best loss and smooth it if smooth_f is specified\n",
    "            if iteration == 0:\n",
    "                self.best_loss = loss\n",
    "            else:\n",
    "                if smooth_f > 0:\n",
    "                    loss = smooth_f * loss + (1 - smooth_f) * self.history[\"loss\"][-1]\n",
    "                if loss < self.best_loss:\n",
    "                    self.best_loss = loss\n",
    "\n",
    "            # Check if the loss has diverged; if it has, stop the test\n",
    "            self.history[\"loss\"].append(loss)\n",
    "            if loss > diverge_th * self.best_loss:\n",
    "                print(\"Stopping early, the loss has diverged\")\n",
    "                break\n",
    "\n",
    "        print(\"Learning rate search finished. See the graph with {finder_name}.plot()\")\n",
    "\n",
    "    def _set_learning_rate(self, new_lrs):\n",
    "        if not isinstance(new_lrs, list):\n",
    "            new_lrs = [new_lrs] * len(self.optimizer.param_groups)\n",
    "        if len(new_lrs) != len(self.optimizer.param_groups):\n",
    "            raise ValueError(\n",
    "                \"Length of `new_lrs` is not equal to the number of parameter groups \"\n",
    "                + \"in the given optimizer\"\n",
    "            )\n",
    "\n",
    "        for param_group, new_lr in zip(self.optimizer.param_groups, new_lrs):\n",
    "            param_group[\"lr\"] = new_lr\n",
    "\n",
    "    def _check_for_scheduler(self):\n",
    "        for param_group in self.optimizer.param_groups:\n",
    "            if \"initial_lr\" in param_group:\n",
    "                raise RuntimeError(\"Optimizer already has a scheduler attached to it\")\n",
    "\n",
    "    def _train_batch(self, train_iter, accumulation_steps, non_blocking_transfer=True):\n",
    "        self.model.train()\n",
    "        total_loss = None  # for late initialization\n",
    "        train_iter = self.accelerator.prepare(train_iter)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        for i in range(accumulation_steps):\n",
    "            train_iter = next(iter(train_iter))\n",
    "            inputs = train_iter\n",
    "\n",
    "            # Forward pass\n",
    "            model_outputs = self.model(**inputs)\n",
    "            loss, outputs = model_outputs.loss, model_outputs.logits\n",
    "\n",
    "            # Loss should be averaged in each step\n",
    "            loss /= accumulation_steps\n",
    "\n",
    "            # Backward pass\n",
    "            self.accelerator.backward(loss)\n",
    "\n",
    "            if total_loss is None:\n",
    "                total_loss = loss\n",
    "            else:\n",
    "                total_loss += loss\n",
    "\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return total_loss.item()\n",
    "\n",
    "    def _move_to_device(self, inputs, labels, non_blocking=True):\n",
    "        def move(obj, device, non_blocking=True):\n",
    "            if hasattr(obj, \"to\"):\n",
    "                return obj.to(device, non_blocking=non_blocking)\n",
    "            elif isinstance(obj, tuple):\n",
    "                return tuple(move(o, device, non_blocking) for o in obj)\n",
    "            elif isinstance(obj, list):\n",
    "                return [move(o, device, non_blocking) for o in obj]\n",
    "            elif isinstance(obj, dict):\n",
    "                return {k: move(o, device, non_blocking) for k, o in obj.items()}\n",
    "            else:\n",
    "                return obj\n",
    "\n",
    "        inputs = move(inputs, self.device, non_blocking=non_blocking)\n",
    "        labels = move(labels, self.device, non_blocking=non_blocking)\n",
    "        return inputs, labels\n",
    "\n",
    "    def _validate(self, val_iter, non_blocking_transfer=True):\n",
    "        # Set model to evaluation mode and disable gradient computation\n",
    "        running_loss = 0\n",
    "        self.model.eval()\n",
    "        val_iter = self.accelerator.prepare(val_iter)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in val_iter:\n",
    "                # Move data to the correct device\n",
    "                inputs, labels = batch, batch['labels']\n",
    "\n",
    "                # Forward pass and loss computation\n",
    "                model_outputs = self.model(**inputs)\n",
    "                loss, outputs = model_outputs.loss, model_outputs.logits\n",
    "                running_loss += loss.item() * len(labels)\n",
    "\n",
    "        return running_loss / len(val_iter.dataset)\n",
    "\n",
    "    def plot(\n",
    "        self,\n",
    "        skip_start=10,\n",
    "        skip_end=5,\n",
    "        log_lr=True,\n",
    "        show_lr=None,\n",
    "        ax=None,\n",
    "        suggest_lr=True,\n",
    "    ):\n",
    "        \"\"\"Plots the learning rate range test.\n",
    "        Arguments:\n",
    "            skip_start (int, optional): number of batches to trim from the start.\n",
    "                Default: 10.\n",
    "            skip_end (int, optional): number of batches to trim from the start.\n",
    "                Default: 5.\n",
    "            log_lr (bool, optional): True to plot the learning rate in a logarithmic\n",
    "                scale; otherwise, plotted in a linear scale. Default: True.\n",
    "            show_lr (float, optional): if set, adds a vertical line to visualize the\n",
    "                specified learning rate. Default: None.\n",
    "            ax (matplotlib.axes.Axes, optional): the plot is created in the specified\n",
    "                matplotlib axes object and the figure is not be shown. If `None`, then\n",
    "                the figure and axes object are created in this method and the figure is\n",
    "                shown . Default: None.\n",
    "            suggest_lr (bool, optional): suggest a learning rate by\n",
    "                - 'steepest': the point with steepest gradient (minimal gradient)\n",
    "                you can use that point as a first guess for an LR. Default: True.\n",
    "        Returns:\n",
    "            The matplotlib.axes.Axes object that contains the plot,\n",
    "            and the suggested learning rate (if set suggest_lr=True).\n",
    "        \"\"\"\n",
    "\n",
    "        if skip_start < 0:\n",
    "            raise ValueError(\"skip_start cannot be negative\")\n",
    "        if skip_end < 0:\n",
    "            raise ValueError(\"skip_end cannot be negative\")\n",
    "        if show_lr is not None and not isinstance(show_lr, float):\n",
    "            raise ValueError(\"show_lr must be float\")\n",
    "\n",
    "        # Get the data to plot from the history dictionary. Also, handle skip_end=0\n",
    "        # properly so the behaviour is the expected\n",
    "        lrs = self.history[\"lr\"]\n",
    "        losses = self.history[\"loss\"]\n",
    "        if skip_end == 0:\n",
    "            lrs = lrs[skip_start:]\n",
    "            losses = losses[skip_start:]\n",
    "        else:\n",
    "            lrs = lrs[skip_start:-skip_end]\n",
    "            losses = losses[skip_start:-skip_end]\n",
    "\n",
    "        # Create the figure and axes object if axes was not already given\n",
    "        fig = None\n",
    "        if ax is None:\n",
    "            fig, ax = plt.subplots()\n",
    "\n",
    "        # Plot loss as a function of the learning rate\n",
    "        ax.plot(lrs, losses)\n",
    "\n",
    "        # Plot the suggested LR\n",
    "        if suggest_lr:\n",
    "            # 'steepest': the point with steepest gradient (minimal gradient)\n",
    "            print(\"LR suggestion: steepest gradient\")\n",
    "            min_grad_idx = None\n",
    "            try:\n",
    "                min_grad_idx = (np.gradient(np.array(losses))).argmin()\n",
    "            except ValueError:\n",
    "                print(\n",
    "                    \"Failed to compute the gradients, there might not be enough points.\"\n",
    "                )\n",
    "            if min_grad_idx is not None:\n",
    "                print(\"Suggested LR: {:.2E}\".format(lrs[min_grad_idx]))\n",
    "                ax.scatter(\n",
    "                    lrs[min_grad_idx],\n",
    "                    losses[min_grad_idx],\n",
    "                    s=75,\n",
    "                    marker=\"o\",\n",
    "                    color=\"red\",\n",
    "                    zorder=3,\n",
    "                    label=\"steepest gradient\",\n",
    "                )\n",
    "                ax.legend()\n",
    "\n",
    "        if log_lr:\n",
    "            ax.set_xscale(\"log\")\n",
    "        ax.set_xlabel(\"Learning rate\")\n",
    "        ax.set_ylabel(\"Loss\")\n",
    "\n",
    "        if show_lr is not None:\n",
    "            ax.axvline(x=show_lr, color=\"red\")\n",
    "\n",
    "        # Show only if the figure was created internally\n",
    "        if fig is not None:\n",
    "            plt.show()\n",
    "\n",
    "        if suggest_lr and min_grad_idx is not None:\n",
    "            return ax, lrs[min_grad_idx]\n",
    "        else:\n",
    "            return ax\n",
    "        \n",
    "class ExponentialLR(_LRScheduler):\n",
    "    def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n",
    "        self.end_lr = end_lr\n",
    "\n",
    "        if num_iter <= 1:\n",
    "            raise ValueError(\"`num_iter` must be larger than 1\")\n",
    "        self.num_iter = num_iter\n",
    "\n",
    "        super(ExponentialLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        # In earlier Pytorch versions last_epoch starts at -1, while in recent versions\n",
    "        # it starts at 0. We need to adjust the math a bit to handle this. See\n",
    "        # discussion at: https://github.com/davidtvs/pytorch-lr-finder/pull/42\n",
    "#         if PYTORCH_VERSION < version.parse(\"1.1.0\"):\n",
    "#             curr_iter = self.last_epoch + 1\n",
    "#             r = curr_iter / (self.num_iter - 1)\n",
    "#         else:\n",
    "        r = self.last_epoch / (self.num_iter - 1)\n",
    "\n",
    "        return [base_lr * (self.end_lr / base_lr) ** r for base_lr in self.base_lrs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-12T06:41:30.479936Z",
     "iopub.status.busy": "2022-03-12T06:41:30.479286Z",
     "iopub.status.idle": "2022-03-12T06:41:51.154656Z",
     "shell.execute_reply": "2022-03-12T06:41:51.153519Z",
     "shell.execute_reply.started": "2022-03-12T06:41:30.479903Z"
    }
   },
   "outputs": [],
   "source": [
    "class MyModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.out = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "        \n",
    "    def forward(self, **x):\n",
    "        return self.out(**x, return_dict=True)\n",
    "    \n",
    "model = MyModel()\n",
    "criterion = torch.nn.NLLLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-7)\n",
    "lr_finder = LRFinder(model, optimizer, criterion, device=\"cuda\")\n",
    "lr_finder.range_test(train_dl, end_lr=100, num_iter=100)\n",
    "lr_finder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-03T06:23:47.285223Z",
     "iopub.status.busy": "2022-04-03T06:23:47.284665Z",
     "iopub.status.idle": "2022-04-03T06:23:53.741424Z",
     "shell.execute_reply": "2022-04-03T06:23:53.740705Z",
     "shell.execute_reply.started": "2022-04-03T06:23:47.285130Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    AutoFeatureExtractor, \n",
    "    AutoTokenizer, \n",
    "    VisionEncoderDecoderModel,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    Seq2SeqTrainer, \n",
    "    default_data_collator,\n",
    ")\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from pathlib import Path\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-03T06:23:53.743426Z",
     "iopub.status.busy": "2022-04-03T06:23:53.743082Z",
     "iopub.status.idle": "2022-04-03T06:23:53.958403Z",
     "shell.execute_reply": "2022-04-03T06:23:53.957610Z",
     "shell.execute_reply.started": "2022-04-03T06:23:53.743395Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['imgs'])\n",
    "imgs, captions = [], []\n",
    "root_dir = Path(\"../input/flickr8k\")\n",
    "\n",
    "with open(root_dir/\"captions.txt\", \"r\") as f:\n",
    "    content = f.readlines()\n",
    "    \n",
    "for line in content:\n",
    "    line = line.strip().split(\"|\")\n",
    "    if line[1]=='1':\n",
    "        imgs.append(root_dir/\"images\"/line[0])\n",
    "        captions.append(line[-1])\n",
    "        \n",
    "df.loc[:, 'imgs'] = imgs\n",
    "df.loc[:, 'captions'] = captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-03T06:23:53.960194Z",
     "iopub.status.busy": "2022-04-03T06:23:53.959920Z",
     "iopub.status.idle": "2022-04-03T06:23:53.974367Z",
     "shell.execute_reply": "2022-04-03T06:23:53.973606Z",
     "shell.execute_reply.started": "2022-04-03T06:23:53.960157Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-03T06:23:53.976368Z",
     "iopub.status.busy": "2022-04-03T06:23:53.976067Z",
     "iopub.status.idle": "2022-04-03T06:23:56.883786Z",
     "shell.execute_reply": "2022-04-03T06:23:56.883069Z",
     "shell.execute_reply.started": "2022-04-03T06:23:53.976332Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "decoder_checkpoint = \"gpt2\"\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(encoder_checkpoint)\n",
    "tokenizer = AutoTokenizer.from_pretrained(decoder_checkpoint)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-03T06:23:56.885272Z",
     "iopub.status.busy": "2022-04-03T06:23:56.885031Z",
     "iopub.status.idle": "2022-04-03T06:23:56.938983Z",
     "shell.execute_reply": "2022-04-03T06:23:56.938282Z",
     "shell.execute_reply.started": "2022-04-03T06:23:56.885237Z"
    }
   },
   "outputs": [],
   "source": [
    "# maximum length for the captions\n",
    "max_length = 128\n",
    "sample = df.iloc[0]\n",
    "\n",
    "# sample image\n",
    "image = Image.open(sample['imgs']).convert('RGB')\n",
    "# sample caption\n",
    "caption = sample['captions']\n",
    "\n",
    "# apply feature extractor on the sample image\n",
    "inputs = feature_extractor(images=image, return_tensors='pt')\n",
    "# apply tokenizer\n",
    "outputs = tokenizer(\n",
    "            caption, \n",
    "            max_length=max_length, \n",
    "            truncation=True, \n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Inputs:\\n{inputs}\\nOutputs:\\n{outputs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-03T06:23:56.940850Z",
     "iopub.status.busy": "2022-04-03T06:23:56.940576Z",
     "iopub.status.idle": "2022-04-03T06:23:56.949147Z",
     "shell.execute_reply": "2022-04-03T06:23:56.948232Z",
     "shell.execute_reply.started": "2022-04-03T06:23:56.940801Z"
    }
   },
   "outputs": [],
   "source": [
    "class LoadDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.images = df['imgs'].values\n",
    "        self.captions = df['captions'].values\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # everything to return is stored inside this dict\n",
    "        inputs = dict()\n",
    "\n",
    "        # load the image and apply feature_extractor\n",
    "        image_path = str(self.images[idx])\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image = feature_extractor(images=image, return_tensors='pt')\n",
    "\n",
    "        # load the caption and apply tokenizer\n",
    "        caption = self.captions[idx]\n",
    "        labels = tokenizer(\n",
    "            caption, \n",
    "            max_length=max_length, \n",
    "            truncation=True, \n",
    "            padding='max_length',\n",
    "            return_tensors='pt',\n",
    "        )['input_ids'][0]\n",
    "        \n",
    "        # store the inputs and labels in the dict we created\n",
    "        inputs['pixel_values'] = image['pixel_values'].squeeze()   \n",
    "        inputs['labels'] = labels\n",
    "        return inputs\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-03T06:23:57.631327Z",
     "iopub.status.busy": "2022-04-03T06:23:57.630880Z",
     "iopub.status.idle": "2022-04-03T06:23:57.641332Z",
     "shell.execute_reply": "2022-04-03T06:23:57.640610Z",
     "shell.execute_reply.started": "2022-04-03T06:23:57.631290Z"
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(df, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "train_ds = LoadDataset(train_df)\n",
    "test_ds = LoadDataset(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(iter(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-03T06:23:59.398730Z",
     "iopub.status.busy": "2022-04-03T06:23:59.398458Z",
     "iopub.status.idle": "2022-04-03T06:24:23.036386Z",
     "shell.execute_reply": "2022-04-03T06:24:23.035632Z",
     "shell.execute_reply.started": "2022-04-03T06:23:59.398694Z"
    }
   },
   "outputs": [],
   "source": [
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    encoder_checkpoint, \n",
    "    decoder_checkpoint\n",
    ")\n",
    "model.config.decoder_start_token_id = tokenizer.bos_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "# model.config.vocab_size = model.config.decoder.vocab_size\n",
    "model.config.num_beams = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_ds))\n",
    "\n",
    "model(pixel_values=batch['pixel_values'].unsqueeze(0), labels=batch['labels'].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-03T06:24:23.038574Z",
     "iopub.status.busy": "2022-04-03T06:24:23.038127Z",
     "iopub.status.idle": "2022-04-03T06:24:27.722382Z",
     "shell.execute_reply": "2022-04-03T06:24:27.721653Z",
     "shell.execute_reply.started": "2022-04-03T06:24:23.038535Z"
    }
   },
   "outputs": [],
   "source": [
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"image-caption-generator\", # name of the directory to store training outputs\n",
    "    evaluation_strategy=\"epoch\",          # evaluate after each epoch\n",
    "    per_device_train_batch_size=8,        # batch size during training\n",
    "    per_device_eval_batch_size=8,         # batch size during evaluation\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,                    # weight decay for AdamW optimizer\n",
    "    num_train_epochs=5,                   # number of epochs to train\n",
    "    save_strategy='epoch',                # save checkpoints after each epoch\n",
    "    report_to='none',                     # prevents logging to wandb, mlflow...\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model, \n",
    "    tokenizer=feature_extractor, \n",
    "    data_collator=default_data_collator,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=test_ds,\n",
    "    args=training_args,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-03T06:24:27.724012Z",
     "iopub.status.busy": "2022-04-03T06:24:27.723735Z",
     "iopub.status.idle": "2022-04-03T07:01:30.584587Z",
     "shell.execute_reply": "2022-04-03T07:01:30.583844Z",
     "shell.execute_reply.started": "2022-04-03T06:24:27.723976Z"
    }
   },
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-03T07:01:31.391108Z",
     "iopub.status.busy": "2022-04-03T07:01:31.389653Z",
     "iopub.status.idle": "2022-04-03T07:01:31.396449Z",
     "shell.execute_reply": "2022-04-03T07:01:31.394168Z",
     "shell.execute_reply.started": "2022-04-03T07:01:31.391066Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-04-03T07:04:07.503664Z",
     "iopub.status.busy": "2022-04-03T07:04:07.503415Z",
     "iopub.status.idle": "2022-04-03T07:04:07.912817Z",
     "shell.execute_reply": "2022-04-03T07:04:07.911307Z",
     "shell.execute_reply.started": "2022-04-03T07:04:07.503635Z"
    }
   },
   "outputs": [],
   "source": [
    "inputs = test_ds[93]['pixel_values']\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    # uncomment the below line if feature extractor is not applied to the image already\n",
    "    # inputs = feature_extractor(images=inputs, return_tensors='pt').pixel_values\n",
    "\n",
    "    # model prediction \n",
    "    out = model.generate(\n",
    "        inputs.unsqueeze(0).to('cuda'), # move inputs to GPU\n",
    "        num_beams=4, \n",
    "#         max_length=17\n",
    "        )\n",
    "\n",
    "# convert token ids to string format\n",
    "decoded_out = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "print(decoded_out)\n",
    "plt.axis('off')\n",
    "plt.imshow(torch.permute(inputs, (1, 2, 0)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2022-03-05T08:39:17.230887Z",
     "iopub.status.busy": "2022-03-05T08:39:17.230533Z",
     "iopub.status.idle": "2022-03-05T08:39:26.700335Z",
     "shell.execute_reply": "2022-03-05T08:39:26.699205Z",
     "shell.execute_reply.started": "2022-03-05T08:39:17.230847Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q datasets accelerate sacrebleu wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-05T08:39:26.703383Z",
     "iopub.status.busy": "2022-03-05T08:39:26.703032Z",
     "iopub.status.idle": "2022-03-05T08:39:27.389799Z",
     "shell.execute_reply": "2022-03-05T08:39:27.388299Z",
     "shell.execute_reply.started": "2022-03-05T08:39:26.703341Z"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_dataset, load_metric\n",
    "from transformers import AutoTokenizer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from accelerate import Accelerator\n",
    "from tqdm.auto import tqdm\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-05T08:37:21.033625Z",
     "iopub.status.busy": "2022-03-05T08:37:21.032768Z",
     "iopub.status.idle": "2022-03-05T08:37:45.516123Z",
     "shell.execute_reply": "2022-03-05T08:37:45.514955Z",
     "shell.execute_reply.started": "2022-03-05T08:37:21.033580Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\"news_commentary\", lang1=\"en\", lang2=\"fr\")\n",
    "print(raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-05T08:37:45.518566Z",
     "iopub.status.busy": "2022-03-05T08:37:45.518298Z",
     "iopub.status.idle": "2022-03-05T08:37:45.565949Z",
     "shell.execute_reply": "2022-03-05T08:37:45.564826Z",
     "shell.execute_reply.started": "2022-03-05T08:37:45.518536Z"
    }
   },
   "outputs": [],
   "source": [
    "split_datasets = raw_datasets['train'].train_test_split(train_size=0.5, test_size=0.1, seed=42)\n",
    "print(split_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-05T08:37:45.568639Z",
     "iopub.status.busy": "2022-03-05T08:37:45.568260Z",
     "iopub.status.idle": "2022-03-05T08:38:06.683400Z",
     "shell.execute_reply": "2022-03-05T08:38:06.682109Z",
     "shell.execute_reply.started": "2022-03-05T08:37:45.568591Z"
    }
   },
   "outputs": [],
   "source": [
    "# model name\n",
    "checkpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n",
    "# load the tokenizer for the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-05T08:38:06.685568Z",
     "iopub.status.busy": "2022-03-05T08:38:06.685148Z",
     "iopub.status.idle": "2022-03-05T08:38:09.269231Z",
     "shell.execute_reply": "2022-03-05T08:38:09.268006Z",
     "shell.execute_reply.started": "2022-03-05T08:38:06.685518Z"
    }
   },
   "outputs": [],
   "source": [
    "sample = split_datasets['train']['translation'][0]\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-05T08:38:09.271343Z",
     "iopub.status.busy": "2022-03-05T08:38:09.271020Z",
     "iopub.status.idle": "2022-03-05T08:38:09.280483Z",
     "shell.execute_reply": "2022-03-05T08:38:09.279604Z",
     "shell.execute_reply.started": "2022-03-05T08:38:09.271307Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer(sample['en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-05T08:38:09.282461Z",
     "iopub.status.busy": "2022-03-05T08:38:09.281964Z",
     "iopub.status.idle": "2022-03-05T08:38:09.301001Z",
     "shell.execute_reply": "2022-03-05T08:38:09.299468Z",
     "shell.execute_reply.started": "2022-03-05T08:38:09.282424Z"
    }
   },
   "outputs": [],
   "source": [
    "max_length = 128\n",
    "\n",
    "def tokenize(examples):\n",
    "    en_sentences = [sent['en'] for sent in examples['translation']]\n",
    "    fr_sentences = [sent['fr'] for sent in examples['translation']]\n",
    "\n",
    "    # tokenize english sentences\n",
    "    model_inputs = tokenizer(en_sentences, max_length=max_length, truncation=True)\n",
    "\n",
    "    # tokenize french sentences\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(fr_sentences, max_length=max_length, truncation=True)\n",
    "\n",
    "    # add tokenized french sentences as labels\n",
    "    model_inputs['labels'] = labels['input_ids']\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-05T08:28:29.008490Z",
     "iopub.status.busy": "2022-03-05T08:28:29.007938Z",
     "iopub.status.idle": "2022-03-05T08:29:29.036596Z",
     "shell.execute_reply": "2022-03-05T08:29:29.035911Z",
     "shell.execute_reply.started": "2022-03-05T08:28:29.008450Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = split_datasets.map(tokenize, batched=True, remove_columns=['id', 'translation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-05T08:38:09.303360Z",
     "iopub.status.busy": "2022-03-05T08:38:09.302797Z",
     "iopub.status.idle": "2022-03-05T08:38:36.565207Z",
     "shell.execute_reply": "2022-03-05T08:38:36.564344Z",
     "shell.execute_reply.started": "2022-03-05T08:38:09.303319Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "collate_fn = DataCollatorForSeq2Seq(tokenizer, model=model)\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    tokenized_datasets['train'], \n",
    "    batch_size=batch_size, \n",
    "    shuffle=True, \n",
    "    collate_fn=collate_fn\n",
    ")\n",
    "\n",
    "test_dl = DataLoader(\n",
    "    tokenized_datasets['test'], \n",
    "    batch_size=batch_size, \n",
    "    shuffle=False, \n",
    "    collate_fn=collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-05T08:29:41.429267Z",
     "iopub.status.busy": "2022-03-05T08:29:41.429037Z",
     "iopub.status.idle": "2022-03-05T08:29:41.466576Z",
     "shell.execute_reply": "2022-03-05T08:29:41.465366Z",
     "shell.execute_reply.started": "2022-03-05T08:29:41.429239Z"
    }
   },
   "outputs": [],
   "source": [
    "batch = next(iter(train_dl))\n",
    "\n",
    "print(batch.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-05T08:29:41.468157Z",
     "iopub.status.busy": "2022-03-05T08:29:41.467900Z",
     "iopub.status.idle": "2022-03-05T08:29:45.862812Z",
     "shell.execute_reply": "2022-03-05T08:29:45.861810Z",
     "shell.execute_reply.started": "2022-03-05T08:29:41.468124Z"
    }
   },
   "outputs": [],
   "source": [
    "opt = optim.AdamW(model.parameters(), lr=5.34e-6)\n",
    "\n",
    "accelerator = Accelerator()\n",
    "train_dl, test_dl, model, opt = accelerator.prepare(\n",
    "    train_dl, test_dl, model, opt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-05T08:29:45.864350Z",
     "iopub.status.busy": "2022-03-05T08:29:45.864090Z",
     "iopub.status.idle": "2022-03-05T08:29:46.640299Z",
     "shell.execute_reply": "2022-03-05T08:29:46.639636Z",
     "shell.execute_reply.started": "2022-03-05T08:29:45.864315Z"
    }
   },
   "outputs": [],
   "source": [
    "metric = load_metric('sacrebleu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-05T08:29:46.641763Z",
     "iopub.status.busy": "2022-03-05T08:29:46.641452Z",
     "iopub.status.idle": "2022-03-05T08:29:46.656069Z",
     "shell.execute_reply": "2022-03-05T08:29:46.655290Z",
     "shell.execute_reply.started": "2022-03-05T08:29:46.641724Z"
    }
   },
   "outputs": [],
   "source": [
    "prediction = ['So it can happen anywhere.']\n",
    "label = ['So it is happen anywhere.']\n",
    "\n",
    "metric.compute(predictions=prediction, references=[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-05T08:29:46.658129Z",
     "iopub.status.busy": "2022-03-05T08:29:46.657526Z",
     "iopub.status.idle": "2022-03-05T08:29:46.664737Z",
     "shell.execute_reply": "2022-03-05T08:29:46.664051Z",
     "shell.execute_reply.started": "2022-03-05T08:29:46.658093Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_preds_and_labels(preds, labels):\n",
    "    preds = preds.detach().cpu()\n",
    "    labels = labels.detach().cpu()\n",
    "    # replace all -100 with the token id of <pad>\n",
    "    labels = torch.where(labels==-100, tokenizer.pad_token_id, labels)\n",
    "    \n",
    "    # decode all token ids to its string/text format\n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # additional cleaning by removing begining and trailing spaces\n",
    "    decoded_preds = [pred.strip() for pred in decoded_preds]\n",
    "    decoded_labels = [[label.strip()] for label in decoded_labels]\n",
    "    \n",
    "    return decoded_preds, decoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-05T08:29:46.666749Z",
     "iopub.status.busy": "2022-03-05T08:29:46.666095Z",
     "iopub.status.idle": "2022-03-05T08:29:46.675128Z",
     "shell.execute_reply": "2022-03-05T08:29:46.674415Z",
     "shell.execute_reply.started": "2022-03-05T08:29:46.666713Z"
    }
   },
   "outputs": [],
   "source": [
    "def run_training(train_dl):\n",
    "    model.train()\n",
    "    for batch in tqdm(train_dl, total=len(train_dl)):\n",
    "        opt.zero_grad()\n",
    "        out = model(**batch)\n",
    "        accelerator.backward(out.loss)\n",
    "        opt.step()\n",
    "        \n",
    "def run_evaluation(test_dl):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dl, total=len(test_dl)):\n",
    "            # generate predictions one by one\n",
    "            preds = model.generate(\n",
    "                input_ids=batch['input_ids'],\n",
    "                attention_mask=batch['attention_mask'],\n",
    "                max_length=max_length,\n",
    "            )\n",
    "            \n",
    "            # convert target labels and predictions to string format for computing accuracy\n",
    "            preds, labels = process_preds_and_labels(preds, batch['labels'])\n",
    "            # add the target labels and predictions of this batch to seqeval\n",
    "            metric.add_batch(predictions=preds, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-05T08:31:37.350248Z",
     "iopub.status.busy": "2022-03-05T08:31:37.349572Z",
     "iopub.status.idle": "2022-03-05T08:31:45.427326Z",
     "shell.execute_reply": "2022-03-05T08:31:45.426539Z",
     "shell.execute_reply.started": "2022-03-05T08:31:37.350212Z"
    }
   },
   "outputs": [],
   "source": [
    "epochs = 3\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    run_training(train_dl)\n",
    "    \n",
    "    run_evaluation(test_dl)\n",
    "    # calculate BLEU score on test set\n",
    "    test_acc = metric.compute()['score']\n",
    "    \n",
    "    # save the model at the end of epoch\n",
    "    torch.save(model.state_dict(), f\"model-v{epoch}.pt\")\n",
    "    print(f\"epoch: {epoch} test_acc: {test_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "run = wandb.init()\n",
    "artifact = run.use_artifact('bipin/machine-translation/model:v4', type='model')\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "model_file = \"./artifacts/model:v4/model-v2.pt\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n",
    "model.load_state_dict(torch.load(model_file, map_location='cpu'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-05T08:41:31.289927Z",
     "iopub.status.busy": "2022-03-05T08:41:31.289624Z",
     "iopub.status.idle": "2022-03-05T08:41:31.299837Z",
     "shell.execute_reply": "2022-03-05T08:41:31.298847Z",
     "shell.execute_reply.started": "2022-03-05T08:41:31.289898Z"
    }
   },
   "outputs": [],
   "source": [
    "split_datasets['test'][0]['translation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-03-05T08:43:46.955992Z",
     "iopub.status.busy": "2022-03-05T08:43:46.955649Z",
     "iopub.status.idle": "2022-03-05T08:43:47.643204Z",
     "shell.execute_reply": "2022-03-05T08:43:47.642240Z",
     "shell.execute_reply.started": "2022-03-05T08:43:46.955959Z"
    }
   },
   "outputs": [],
   "source": [
    "sample = split_datasets['test'][0]['translation']\n",
    "inputs = sample['en']\n",
    "label = sample['fr']\n",
    "\n",
    "inputs = tokenizer(inputs, return_tensors='pt')\n",
    "out = model.generate(**inputs)\n",
    "\n",
    "# convert token ids to string\n",
    "with tokenizer.as_target_tokenizer():\n",
    "    decoded_out = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Label: \\n\", label)\n",
    "print(\"Prediction: \\n\", decoded_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
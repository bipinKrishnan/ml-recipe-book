
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Named entity recognition</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://github.com/_ner_chapter.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Image segmentation" href="cv.html" />
    <link rel="prev" title="Introduction" href="index.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="index.html">
   Introduction
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  NLP
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Named entity recognition
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Computer vision
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="cv.html">
   Image segmentation
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/_ner_chapter.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/bipinkrishnan/ml-powered-apps"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-named-entity-recognition-ner">
   What is named entity recognition(NER)?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#preparing-the-dataset">
   Preparing the dataset
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#downloading-the-dataset">
     Downloading the dataset
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#understanding-model-inputs">
     Understanding model inputs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#understanding-model-outputs">
     Understanding model outputs
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#preparing-the-dataloader">
   Preparing the dataloader
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training-the-model">
   Training the model
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <section id="named-entity-recognition">
<h1>Named entity recognition<a class="headerlink" href="#named-entity-recognition" title="Permalink to this headline">¶</a></h1>
<p>The plan for this chapter is to:</p>
<ol class="simple">
<li><p>Learn about named entity recognition(NER)</p></li>
<li><p>Train a transformer model for NER</p></li>
<li><p>Build a web app demo(using Gradio) around the trained model to convert a sentence from western context to Indian context.</p></li>
</ol>
<p>If you did not get the last point, here is an example to make it clear:</p>
<p>If we pass the sentence ‘I am going to Paris’ as input, the model will modify the sentence and give the output as ‘I am going to Mumbai’. As you can see, the model identified the word ‘Paris’ and coverted it to ‘Mumbai’ which is more familiar to Indians. The idea for this demo is inspired from this <a class="reference external" href="https://towardsdatascience.com/practical-ai-using-nlp-word-vectors-in-a-novel-way-to-solve-the-problem-of-localization-9de3e4fbf56f">blog post</a>.</p>
<p>Below you can see the final demo that we will be building in this chapter:</p>
<p><img alt="NER Gradio demo" src="_images/ner_gr_demo.png" /></p>
<section id="what-is-named-entity-recognition-ner">
<h2>What is named entity recognition(NER)?<a class="headerlink" href="#what-is-named-entity-recognition-ner" title="Permalink to this headline">¶</a></h2>
<p>At present, there are a wide variety of tasks that machine learning models are capable of doing, named entity recognition or NER is one them. In short the job of a model trained for this task is to identify all the named entities in a given sentence.</p>
<p>Here is a figure showing what the model is expected to do for NER:</p>
<img alt="Named entity recignition" class="bg-primary mb-1 align-center" src="_images/ner_process.png" />
<p>In the above figure, the model recognizes ‘Sarah’ as a person(PER) and ‘London’ as a location(LOC) entity. Since the other words do not belong to any category of entities, no labels are present in the output for those words.</p>
<p>Named entity recognition does not limit to identitfying a person, location or organization, it can also be used for identifying parts of speech of each word in a sentence. The term used to generalize these kinds of tasks is called token classification.</p>
<p>As humans, when we are reading a book, we understand a sentence by reading each word in that, right? Similarly, before passing a sentence into our models, we split them into simpler tokens using something called a tokenizer.</p>
<p>The simplest tokenizer you can think of is splitting a sentence into words as shown below:</p>
<p>Input sentence: <code class="docutils literal notranslate"><span class="pre">This</span> <span class="pre">is</span> <span class="pre">looking</span> <span class="pre">good</span></code></p>
<p>Output tokens: <code class="docutils literal notranslate"><span class="pre">['This',</span> <span class="pre">'is',</span> <span class="pre">'looking',</span> <span class="pre">'good']</span></code></p>
<p>And here is a figure to illustrate the same:</p>
<p><img alt="Named entity recignition tokenizer" src="_images/ner_tokenizer.png" /></p>
<p>We now have a basic understanding of the task and the related terms that we will encounter in this chapter, now let’s talk about the dataset that we will be using for this task.</p>
</section>
<section id="preparing-the-dataset">
<h2>Preparing the dataset<a class="headerlink" href="#preparing-the-dataset" title="Permalink to this headline">¶</a></h2>
<section id="downloading-the-dataset">
<h3>Downloading the dataset<a class="headerlink" href="#downloading-the-dataset" title="Permalink to this headline">¶</a></h3>
<p>We will be using the <a class="reference external" href="https://huggingface.co/datasets/conllpp">conllpp</a> which can be directly downloaded using the huggingface datasets library using the code shown below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="n">raw_datasets</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;conllpp&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>If we print <code class="docutils literal notranslate"><span class="pre">raw_datasets</span></code>, we can see the structure of the dataset:</p>
<p><img alt="ner_data_dict" src="_images/ner_data_dict.png" /></p>
<p>The dataset is already split into train, validation and test sets where each set has an id, tokens, pos tags, chunk tags and ner tags as features.</p>
<p>Can you guess the features that we will be using for this chapter?</p>
<p>If you guessed it correctly, <code class="docutils literal notranslate"><span class="pre">tokens</span></code> and <code class="docutils literal notranslate"><span class="pre">ner_tags</span></code> are the only features we require for the present use case.</p>
<p>This is how the first 5 rows of the training dataset will look like if it were a pandas dataframe:</p>
<p><img alt="ner_data_frame" src="_images/ner_data_frame.png" /></p>
<p>Now lets talk about the columns that we will be using for our task. The <code class="docutils literal notranslate"><span class="pre">tokens</span></code> column will be our inputs to the model and <code class="docutils literal notranslate"><span class="pre">ner_tags</span></code> is what the model should predict.</p>
</section>
<section id="understanding-model-inputs">
<h3>Understanding model inputs<a class="headerlink" href="#understanding-model-inputs" title="Permalink to this headline">¶</a></h3>
<p>As you can see, the column <code class="docutils literal notranslate"><span class="pre">tokens</span></code> are not sentences, instead it is a list of words(which is a kind of tokenization as we discussed earlier), so from now on we can call this as pre-tokenized inputs. Even though it’s already split into words, we cannot directly feed it to our model. Our model uses something called <strong>sub-word tokenizer</strong>, which is nothing but splitting a word into multiple subwords.</p>
<p>Let’s take an example and make it clear:</p>
<p>Input sentence: <code class="docutils literal notranslate"><span class="pre">&quot;This</span> <span class="pre">is</span> <span class="pre">the</span> <span class="pre">pytorch</span> <span class="pre">code&quot;</span></code></p>
<p>For the above sentence, a simple tokenizer that splits a sentence into words will give the following output: <code class="docutils literal notranslate"><span class="pre">['This',</span> <span class="pre">'is',</span> <span class="pre">'the',</span> <span class="pre">'pytorch',</span> <span class="pre">'code']</span></code></p>
<p>But a sub-word tokenizer may split some words into even simpler sub-words, just like this:</p>
<p><code class="docutils literal notranslate"><span class="pre">['This',</span> <span class="pre">'is',</span> <span class="pre">'the',</span> <span class="pre">'p',</span> <span class="pre">'##yt',</span> <span class="pre">'##or',</span> <span class="pre">'##ch',</span> <span class="pre">'code']</span></code></p>
<p>As you can see, some words like pytorch which is not commonly seen in sentences are split into multiple sub-words. Also note that except for the starting token ‘p’, all the sub-words for the word ‘pytorch’ starts with a ‘##’.</p>
<p>The model that we will be using for this task is <code class="docutils literal notranslate"><span class="pre">bert-base-cased</span></code> which is bert model that treats cased and uncased words differently. This specific model will be helpful for our task because when writing the name of a person or a location, we always start with an upper-case letter which will make the job of our model way more easier while identifying named entities.</p>
<p>Now let’s write some code to tokenize each row in the <code class="docutils literal notranslate"><span class="pre">tokens</span></code> column using the <code class="docutils literal notranslate"><span class="pre">AutoTokenizer</span></code> module in <code class="docutils literal notranslate"><span class="pre">transformers</span></code> library.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>  <span class="c1"># to tokenize the inputs</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="s1">&#39;bert-base-cased&#39;</span>

<span class="c1"># load tokenizer for our checkpoint</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>

<span class="c1"># first row of the training set</span>
<span class="n">train_row_1</span> <span class="o">=</span> <span class="n">raw_datasets</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p>Now let’s pass in the <code class="docutils literal notranslate"><span class="pre">tokens</span></code> column of the first row into our tokenizer. Since our <code class="docutils literal notranslate"><span class="pre">tokens</span></code> column contain pre-tokenized inputs, we need to set the argument <code class="docutils literal notranslate"><span class="pre">is_split_into_words</span></code> to <code class="docutils literal notranslate"><span class="pre">True</span></code> while calling the tokenizer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">train_row_1</span><span class="p">[</span><span class="s1">&#39;tokens&#39;</span><span class="p">],</span> <span class="n">is_split_into_words</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>If you print the <code class="docutils literal notranslate"><span class="pre">inputs</span></code>, you can see that our tokenizer has tokenized the words(as shown below) in such a way that it contain all the information to directly feed into our transformer model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">&#39;input_ids&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">101</span><span class="p">,</span> <span class="mi">7270</span><span class="p">,</span> <span class="mi">22961</span><span class="p">,</span> <span class="mi">1528</span><span class="p">,</span> <span class="mi">1840</span><span class="p">,</span> <span class="mi">1106</span><span class="p">,</span> <span class="mi">21423</span><span class="p">,</span> <span class="mi">1418</span><span class="p">,</span> <span class="mi">2495</span><span class="p">,</span> <span class="mi">12913</span><span class="p">,</span> <span class="mi">119</span><span class="p">,</span> <span class="mi">102</span><span class="p">],</span> <span class="s1">&#39;token_type_ids&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> 
    <span class="s1">&#39;attention_mask&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="understanding-model-outputs">
<h3>Understanding model outputs<a class="headerlink" href="#understanding-model-outputs" title="Permalink to this headline">¶</a></h3>
<p>As we’ve said earlier, <code class="docutils literal notranslate"><span class="pre">ner_tags</span></code> is the column that we will use as labels/outputs for our model. Here is the first 5 rows of this column:</p>
<img alt="ner_tags" class="bg-primary mb-1 align-center" src="_images/ner_tags.png" />
<p>As you can see, it’s a list of numbers. Let’s see what does these numbers actually mean.</p>
<p>You will get all the information about each feature in the dataset using <code class="docutils literal notranslate"><span class="pre">.features</span></code> method. Once you filter out only the <code class="docutils literal notranslate"><span class="pre">ner_tags</span></code> feature from that, you will get complete information about it, just like shown below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">raw_datasets</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">features</span><span class="p">[</span><span class="s1">&#39;ner_tags&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>And you will get an output like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Sequence</span><span class="p">(</span><span class="n">feature</span><span class="o">=</span><span class="n">ClassLabel</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;O&#39;</span><span class="p">,</span> <span class="s1">&#39;B-PER&#39;</span><span class="p">,</span> <span class="s1">&#39;I-PER&#39;</span><span class="p">,</span> <span class="s1">&#39;B-ORG&#39;</span><span class="p">,</span> <span class="s1">&#39;I-ORG&#39;</span><span class="p">,</span> <span class="s1">&#39;B-LOC&#39;</span><span class="p">,</span> <span class="s1">&#39;I-LOC&#39;</span><span class="p">,</span> <span class="s1">&#39;B-MISC&#39;</span><span class="p">,</span> <span class="s1">&#39;I-MISC&#39;</span><span class="p">],</span> <span class="n">names_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span> <span class="n">length</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
<p>Now, it’s clear that there are 9 different classes/entities inside the <code class="docutils literal notranslate"><span class="pre">ner_tags</span></code> and the name of each label is present inside <code class="docutils literal notranslate"><span class="pre">names</span></code>.</p>
<ol class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">PER</span></code>, <code class="docutils literal notranslate"><span class="pre">ORG</span></code>, <code class="docutils literal notranslate"><span class="pre">LOC</span></code> and <code class="docutils literal notranslate"><span class="pre">MISC</span></code> represents a person, organisation, location and miscellaneous entities respectively. The only one remaining is <code class="docutils literal notranslate"><span class="pre">O</span></code> which we can use to represent words that doesn’t belong to any entity.</p></li>
<li><p>You might have noticed the <code class="docutils literal notranslate"><span class="pre">B-</span></code> and <code class="docutils literal notranslate"><span class="pre">I-</span></code> prefixes for these entities, those represent whether a word is at the begining or inside an entity. Let me give you an example and make it clear.</p></li>
</ol>
<p>If the input is like this: <code class="docutils literal notranslate"><span class="pre">'My</span> <span class="pre">name</span> <span class="pre">is</span> <span class="pre">Roy</span> <span class="pre">Lee</span> <span class="pre">and</span> <span class="pre">I</span> <span class="pre">am</span> <span class="pre">from</span> <span class="pre">New</span> <span class="pre">York'</span></code></p>
<p>The corresponding labels should be: <code class="docutils literal notranslate"><span class="pre">['O',</span> <span class="pre">'O',</span> <span class="pre">'O',</span> <span class="pre">'B-PER',</span> <span class="pre">'I-PER',</span> <span class="pre">'O',</span> <span class="pre">'O',</span> <span class="pre">'O',</span> <span class="pre">'O',</span> <span class="pre">'B-LOC',</span> <span class="pre">'I-LOC']</span></code> where each label in the list corresponds to each word in the input sentence.</p>
<p>As you can see, the words ‘Roy’ and ‘New’ are the words at the begining of a person and a location, so they are given the labels as <code class="docutils literal notranslate"><span class="pre">B-PER</span></code> and <code class="docutils literal notranslate"><span class="pre">B-LOC</span></code> respectively. Whereas, the words ‘Lee’ and ‘York’ are not at the begining but inside a person and a location entity, so they are given the label <code class="docutils literal notranslate"><span class="pre">I-PER</span></code> and <code class="docutils literal notranslate"><span class="pre">I-LOC</span></code> respectively.</p>
<p>Lastly, all the words that does not belong to any entity is mapped to <code class="docutils literal notranslate"><span class="pre">O</span></code> tag.</p>
<p>Finally to wrap up this part, we will create a dictionary containing the id to label mapping.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># store all the ner labels</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">raw_datasets</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">features</span><span class="p">[</span><span class="s1">&#39;ner_tags&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">feature</span><span class="o">.</span><span class="n">names</span>

<span class="c1"># number of classes/ner tags -&gt; [0, 1, 2, 3, 4, 5, 6, 7, 8]</span>
<span class="n">ids</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">))</span>

<span class="c1"># id to label mapping </span>
<span class="n">id2label</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">labels</span><span class="p">))</span>
</pre></div>
</div>
<p>Now we have an id to label mapping as shown below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span> <span class="p">{</span> 
   <span class="mi">0</span><span class="p">:</span> <span class="s1">&#39;O&#39;</span><span class="p">,</span>
   <span class="mi">1</span><span class="p">:</span> <span class="s1">&#39;B-PER&#39;</span><span class="p">,</span>
   <span class="mi">2</span><span class="p">:</span> <span class="s1">&#39;I-PER&#39;</span><span class="p">,</span>
   <span class="mi">3</span><span class="p">:</span> <span class="s1">&#39;B-ORG&#39;</span><span class="p">,</span>
   <span class="mi">4</span><span class="p">:</span> <span class="s1">&#39;I-ORG&#39;</span><span class="p">,</span>
   <span class="mi">5</span><span class="p">:</span> <span class="s1">&#39;B-LOC&#39;</span><span class="p">,</span>
   <span class="mi">6</span><span class="p">:</span> <span class="s1">&#39;I-LOC&#39;</span><span class="p">,</span>
   <span class="mi">7</span><span class="p">:</span> <span class="s1">&#39;B-MISC&#39;</span><span class="p">,</span>
   <span class="mi">8</span><span class="p">:</span> <span class="s1">&#39;I-MISC&#39;</span> 
 <span class="p">}</span>
</pre></div>
</div>
<p>As we have an understanding of the inputs as well outputs/labels of the dataset that we are going to use, it’s time to do some preprocessing and create a train, validation and test dataloader.</p>
</section>
</section>
<section id="preparing-the-dataloader">
<h2>Preparing the dataloader<a class="headerlink" href="#preparing-the-dataloader" title="Permalink to this headline">¶</a></h2>
<p>We need to complete the following tasks before wrapping everything inside a dataloader:</p>
<ol class="simple">
<li><p>Tokenize the inputs(<code class="docutils literal notranslate"><span class="pre">tokens</span></code> column)</p></li>
<li><p>Align the tokens and labels</p></li>
</ol>
<p>The first point is strainght forward and you must have got it. It’s just tokenizing our inputs as we did earlier in this chapter. Let’s talk about the second part.</p>
<p>Let’s take the first example from our <code class="docutils literal notranslate"><span class="pre">tokens</span></code> column in our training set and explain this concept.</p>
<p>This is inputs and outputs for the first row:</p>
<p>Inputs: <code class="docutils literal notranslate"><span class="pre">['EU',</span> <span class="pre">'rejects',</span> <span class="pre">'German',</span> <span class="pre">'call',</span> <span class="pre">'to',</span> <span class="pre">'boycott',</span> <span class="pre">'British',</span> <span class="pre">'lamb',</span> <span class="pre">'.']</span></code></p>
<p>Outputs: <code class="docutils literal notranslate"><span class="pre">[3,</span> <span class="pre">0,</span> <span class="pre">7,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">7,</span> <span class="pre">0,</span> <span class="pre">0]</span></code></p>
<p>Each word in the input list corresponds to each label in the outputs list, so, both of their lengths are the same. We cannot pass the inputs as it is because those are strings and we need to use our tokenizer and convert them to integers as we’ve shown earlier in the chapter. For the outputs, since they are already available as integers, we don’t need to bother about them for now.</p>
<p>Let’s tokenize the inputs and see how the tokens look.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">train_row_1</span><span class="p">[</span><span class="s1">&#39;tokens&#39;</span><span class="p">],</span> <span class="n">is_split_into_words</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">tokens</span><span class="p">())</span>
</pre></div>
</div>
<p>These are the output tokens:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="s1">&#39;[CLS]&#39;</span><span class="p">,</span> <span class="s1">&#39;EU&#39;</span><span class="p">,</span> <span class="s1">&#39;rejects&#39;</span><span class="p">,</span> <span class="s1">&#39;German&#39;</span><span class="p">,</span> <span class="s1">&#39;call&#39;</span><span class="p">,</span> <span class="s1">&#39;to&#39;</span><span class="p">,</span> <span class="s1">&#39;boycott&#39;</span><span class="p">,</span> <span class="s1">&#39;British&#39;</span><span class="p">,</span> <span class="s1">&#39;la&#39;</span><span class="p">,</span> <span class="s1">&#39;##mb&#39;</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="s1">&#39;[SEP]&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>As you can see there are two special tokens <code class="docutils literal notranslate"><span class="pre">[CLS]</span></code> and <code class="docutils literal notranslate"><span class="pre">[SEP]</span></code> at the begining and end of the sentence, those are specific to the model that we are using. <code class="docutils literal notranslate"><span class="pre">[CLS]</span></code> is special token put at the start of an input sentence whereas <code class="docutils literal notranslate"><span class="pre">[SEP]</span></code> is used to seperate sentences.
Since we have only a single sentence <code class="docutils literal notranslate"><span class="pre">[SEP]</span></code> is present after the sentence ends.</p>
<p>Another problem is that the word ‘lamb’ is plit into ‘la’ and ‘##mb’ by the tokenizer. Now the length of our input tokens is 12 whereas the length of the labels is 9 because our tokenizer added a <code class="docutils literal notranslate"><span class="pre">[CLS]</span></code>, <code class="docutils literal notranslate"><span class="pre">[SEP]</span></code> and <code class="docutils literal notranslate"><span class="pre">##mb</span></code> to our inputs.</p>
<p>For training the model, each word should have a label assigned to it, so we need to align the tokens and labels in such a way that both their lengths are the same.</p>
<p>Fortunately, even after tokenization and splitting words into multiple sub-words, we could get the word ids of each token using <code class="docutils literal notranslate"><span class="pre">.word_ids()</span></code> method. Here is an example showing the same.</p>
<p>For the inputs <code class="docutils literal notranslate"><span class="pre">['EU',</span> <span class="pre">'rejects',</span> <span class="pre">'German',</span> <span class="pre">'call',</span> <span class="pre">'to',</span> <span class="pre">'boycott',</span> <span class="pre">'British',</span> <span class="pre">'lamb',</span> <span class="pre">'.']</span></code>, the word ids will be the index of the word in the list.</p>
<p>So, if we tokenize the above inputs using the tokenizer and take the word ids, this is what we get: <code class="docutils literal notranslate"><span class="pre">[None,</span> <span class="pre">0,</span> <span class="pre">1,</span> <span class="pre">2,</span> <span class="pre">3,</span> <span class="pre">4,</span> <span class="pre">5,</span> <span class="pre">6,</span> <span class="pre">7,</span> <span class="pre">7,</span> <span class="pre">8,</span> <span class="pre">None]</span></code></p>
<p>The two <code class="docutils literal notranslate"><span class="pre">None</span></code> values at the start and end represents <code class="docutils literal notranslate"><span class="pre">[CLS]</span></code> and <code class="docutils literal notranslate"><span class="pre">[SEP]</span></code> tokens. The rest of the integers represent the word ids for each token. You can clearly see that the only word id that is repeating twice is 7 which is the word id for ‘lamb’. Since it’s split into ‘la’ and ‘##mb’, both of them have the same word ids.</p>
<p>Here is an illustrated diagram to make the above process clear:</p>
<img alt="ner_word_ids" class="bg-primary mb-1 align-center" src="_images/ner_word_ids.png" />
<p>Now let’s write a simple function to align labels with tokens.</p>
<p>The function will take a list word ids and its corresponding ner labels as arguments and then the following things happen. We loop through each word id in the provided list and checks the below conditions:</p>
<ol class="simple">
<li><p>The first condition is to check whether the current word id is not equal to the previous word id(which means that these two tokens does not belong to the same word).</p></li>
<li><p>The second condition checks whether the token is a special token which will have a word id as <code class="docutils literal notranslate"><span class="pre">None</span></code>.</p></li>
<li><p>If the above two conditions are not satisfied(which means that the token is part of a word as well as not a special token), the last part of the rule is executed.</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">align_tokens_and_labels</span><span class="p">(</span><span class="n">word_ids</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="n">previous_word_id</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">new_labels</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">for</span> <span class="n">word_id</span> <span class="ow">in</span> <span class="n">word_ids</span><span class="p">:</span>
        
        <span class="k">if</span> <span class="n">word_id</span><span class="o">!=</span><span class="n">previous_word_id</span><span class="p">:</span>
            <span class="n">label</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span> <span class="k">if</span> <span class="n">word_id</span><span class="o">==</span><span class="kc">None</span> <span class="k">else</span> <span class="n">labels</span><span class="p">[</span><span class="n">word_id</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">word_id</span><span class="o">==</span><span class="kc">None</span><span class="p">:</span>
            <span class="n">label</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">label</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">word_id</span><span class="p">]</span>

            <span class="c1"># checks if the ner label is of the form B-XXX</span>
            <span class="k">if</span> <span class="n">label</span><span class="o">%</span><span class="mi">2</span><span class="o">==</span><span class="mi">1</span><span class="p">:</span>
                <span class="c1"># converts the label from B-XXX to I-XXX</span>
                <span class="n">label</span> <span class="o">+=</span> <span class="mi">1</span>
                
        <span class="n">previous_word_id</span> <span class="o">=</span> <span class="n">word_id</span>
        <span class="n">new_labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
                
    <span class="k">return</span> <span class="n">new_labels</span>
</pre></div>
</div>
<p>Let’s test the function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ner_labels</span> <span class="o">=</span> <span class="n">train_row_1</span><span class="p">[</span><span class="s1">&#39;ner_tags&#39;</span><span class="p">]</span>

<span class="c1"># tokenized inputs</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">train_row_1</span><span class="p">[</span><span class="s1">&#39;tokens&#39;</span><span class="p">],</span> <span class="n">is_split_into_words</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">word_ids</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">word_ids</span><span class="p">()</span>

<span class="n">align_tokens_and_labels</span><span class="p">(</span><span class="n">word_ids</span><span class="p">,</span> <span class="n">ner_labels</span><span class="p">)</span>
</pre></div>
</div>
<p>Here is the output labels:
<code class="docutils literal notranslate"><span class="pre">[-100,</span> <span class="pre">3,</span> <span class="pre">0,</span> <span class="pre">7,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">7,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">-100]</span></code></p>
<p>Now we can write a single function to apply this to every example in our dataset. We will pass a batch of examples from our dataset to this function and the function will loop through example in the batch and apply <code class="docutils literal notranslate"><span class="pre">align_tokens_and_labels</span></code> and returns the tokenized inputs and corresponding outputs/labels in the required format.</p>
<p>We also truncate our tokens to a length of 512, any input example that has a length higher than this are shortened/truncated to 512. The default maximum length set inside the tokenizer is 512, so we just need to set <code class="docutils literal notranslate"><span class="pre">truncation=True</span></code> while tokenizing the inputs.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We can tokenize a group of input examples together and get the word ids using indexing.</p>
<p>For example, if you’ve tokenized 3 sentences together, you can get the word ids of the first sentence using <code class="docutils literal notranslate"><span class="pre">inputs.word_ids(0)</span></code>.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">prepare_inputs_and_labels</span><span class="p">(</span><span class="n">ds</span><span class="p">):</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">ds</span><span class="p">[</span><span class="s1">&#39;tokens&#39;</span><span class="p">],</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">is_split_into_words</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">labels_batch</span> <span class="o">=</span> <span class="n">ds</span><span class="p">[</span><span class="s1">&#39;ner_tags&#39;</span><span class="p">]</span>
    
    <span class="n">new_labels</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># loop through each example in the batch</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">labels_batch</span><span class="p">):</span>
        <span class="c1"># extract the word ids using the index</span>
        <span class="n">word_ids</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">word_ids</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
        <span class="n">new_label</span> <span class="o">=</span> <span class="n">align_tokens_and_labels</span><span class="p">(</span><span class="n">word_ids</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">new_labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_label</span><span class="p">)</span>
        
    <span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_labels</span>
    <span class="k">return</span> <span class="n">inputs</span>
</pre></div>
</div>
<p>Now let’s apply this function to our dataset as below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">prepared_datasets</span> <span class="o">=</span> <span class="n">raw_datasets</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
    <span class="n">prepare_inputs_and_labels</span><span class="p">,</span> 
    <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
    <span class="n">remove_columns</span><span class="o">=</span><span class="n">raw_datasets</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">column_names</span>
<span class="p">)</span>
</pre></div>
</div>
<p>We used the <code class="docutils literal notranslate"><span class="pre">.map</span></code> method and set <code class="docutils literal notranslate"><span class="pre">batched=True</span></code> to map our function to each batch in our dataset. Our final prepared dataset will only contain the following features: <code class="docutils literal notranslate"><span class="pre">['input_ids',</span> <span class="pre">'token_type_ids',</span> <span class="pre">'attention_mask',</span> <span class="pre">'labels']</span></code>, all other features are removed.</p>
<p>Now let’s wrap our datasets inside a pytorch dataloader as shown below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># to pad the inputs and labels in a batch to same size</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DataCollatorForTokenClassification</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">collate_fn</span> <span class="o">=</span> <span class="n">DataCollatorForTokenClassification</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>

<span class="c1"># training dataloader</span>
<span class="n">train_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">prepared_datasets</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">],</span> 
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> 
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
    <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_fn</span>
    <span class="p">)</span>

<span class="c1"># validation dataloader</span>
<span class="n">val_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">prepared_datasets</span><span class="p">[</span><span class="s1">&#39;validation&#39;</span><span class="p">],</span> 
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> 
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
    <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_fn</span>
    <span class="p">)</span>

<span class="c1"># test dataloader</span>
<span class="n">test_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">prepared_datasets</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">],</span> 
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> 
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
    <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_fn</span>
    <span class="p">)</span>
</pre></div>
</div>
</section>
<section id="training-the-model">
<h2>Training the model<a class="headerlink" href="#training-the-model" title="Permalink to this headline">¶</a></h2>
<p>As our dataloaders are in place, let’s discuss the steps to train our model. The steps are almost similar to the normal training loop that we use for training other models with pytorch.</p>
<p>First we will write the code to create the model and optimizer for our training:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">optim</span>
<span class="c1"># token classification model</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForTokenClassification</span>

<span class="c1"># load the pretrained model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForTokenClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">checkpoint</span><span class="p">,</span> 
    <span class="n">num_labels</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
    <span class="p">)</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1.23e-4</span><span class="p">)</span>
</pre></div>
</div>
<p>The model will be loaded and the number of units in the classification part of the model will be replaced by the value provided to <code class="docutils literal notranslate"><span class="pre">num_labels</span></code>, which in our case will be 9. The learning rate is obtained using the this <a class="reference external" href="https://github.com/davidtvs/pytorch-lr-finder">learning rate finder</a>. I made some hacky tweaks to make it work for this specific application.</p>
<p>We will obviously use a GPU for training the model, so we need to move our dataloaders, model and the optimizer to it as well. We will use huggingface’s <code class="docutils literal notranslate"><span class="pre">accelerate</span></code> library for this. If not already installed you can do it by running the command <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">accelerate</span></code> from your terminal.</p>
<p>With <code class="docutils literal notranslate"><span class="pre">accelerate</span></code>, moving everything to GPU is as simple as this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">accelerate</span> <span class="kn">import</span> <span class="n">Accelerator</span>

<span class="n">accelerator</span> <span class="o">=</span> <span class="n">Accelerator</span><span class="p">()</span>
<span class="n">train_dl</span><span class="p">,</span> <span class="n">val_dl</span><span class="p">,</span> <span class="n">test_dl</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">opt</span> <span class="o">=</span> <span class="n">accelerator</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span>
    <span class="n">train_dl</span><span class="p">,</span> 
    <span class="n">val_dl</span><span class="p">,</span> 
    <span class="n">test_dl</span><span class="p">,</span> 
    <span class="n">model</span><span class="p">,</span> 
    <span class="n">opt</span>
<span class="p">)</span>
</pre></div>
</div>
<p>We now need a metric to measure the performance of our model after each epoch. We will use the <code class="docutils literal notranslate"><span class="pre">seqeval</span></code> framework for this, using which we can get the f1-score, precision, recall and overall accuracy. You can install it by running <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">seqeval</span></code> on your terminal.</p>
<p>For calculating the metrics, we need to pass in the predictions and corresponding labels in string format to <code class="docutils literal notranslate"><span class="pre">seqeval</span></code>, just like shown below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_metric</span>

<span class="n">metric</span> <span class="o">=</span> <span class="n">load_metric</span><span class="p">(</span><span class="s1">&#39;seqeval&#39;</span><span class="p">)</span>

<span class="n">targets</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;O&#39;</span><span class="p">,</span> <span class="s1">&#39;B-PER&#39;</span><span class="p">,</span> <span class="s1">&#39;I-PER&#39;</span><span class="p">,</span> <span class="s1">&#39;O&#39;</span><span class="p">,</span> <span class="s1">&#39;O&#39;</span><span class="p">,</span> <span class="s1">&#39;O&#39;</span><span class="p">]</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;O&#39;</span><span class="p">,</span> <span class="s1">&#39;B-PER&#39;</span><span class="p">,</span> <span class="s1">&#39;O&#39;</span><span class="p">,</span> <span class="s1">&#39;O&#39;</span><span class="p">,</span> <span class="s1">&#39;O&#39;</span><span class="p">,</span> <span class="s1">&#39;O&#39;</span><span class="p">]</span>

<span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="p">[</span><span class="n">predictions</span><span class="p">],</span> <span class="n">references</span><span class="o">=</span><span class="p">[</span><span class="n">targets</span><span class="p">])</span>
</pre></div>
</div>
<p>Running the above code will return this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s1">&#39;PER&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;precision&#39;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span> <span class="s1">&#39;recall&#39;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span> <span class="s1">&#39;f1&#39;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span> <span class="s1">&#39;number&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">},</span>
 <span class="s1">&#39;overall_precision&#39;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
 <span class="s1">&#39;overall_recall&#39;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
 <span class="s1">&#39;overall_f1&#39;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
 <span class="s1">&#39;overall_accuracy&#39;</span><span class="p">:</span> <span class="mf">0.8333333333333334</span><span class="p">}</span>
</pre></div>
</div>
<p>Of the above outputs, we will only take into account the <code class="docutils literal notranslate"><span class="pre">overall_accuracy</span></code> and print it after each epoch to measure the performance of our model on the validation set.</p>
<p>We need to the predictions and labels in string format to calculate the overall accuracy, so lets write a function to convert them from numbers to string format.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">process_preds_and_labels</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">preds</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">preds</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>

    <span class="n">true_targets</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">[</span><span class="n">labels</span><span class="p">[</span><span class="n">t</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">target</span> <span class="k">if</span> <span class="n">t</span><span class="o">!=-</span><span class="mi">100</span><span class="p">]</span> 
        <span class="k">for</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">targets</span>
        <span class="p">]</span>
    <span class="n">true_preds</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">[</span><span class="n">labels</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span> <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="k">if</span> <span class="n">t</span><span class="o">!=-</span><span class="mi">100</span><span class="p">]</span> 
        <span class="k">for</span> <span class="n">pred</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
        <span class="p">]</span>

    <span class="k">return</span> <span class="n">true_preds</span><span class="p">,</span> <span class="n">true_targets</span>
</pre></div>
</div>
<p>Now let’s write a function for our training loop which takes a dataloader as input and gets the prediction from the model, calculates the loss and finally does a backward pass and steps the optimizer. For the backward pass we will use <code class="docutils literal notranslate"><span class="pre">accelerator.backward(model.loss)</span></code> instead of <code class="docutils literal notranslate"><span class="pre">model.loss.backward()</span></code> as we’ve used <code class="docutils literal notranslate"><span class="pre">accelerate</span></code> to move everything to GPU.</p>
<p>The output of the model will contain the logits/predictions as well as the loss and can be accessed by <code class="docutils literal notranslate"><span class="pre">model.logits</span></code> and <code class="docutils literal notranslate"><span class="pre">model.loss</span></code> respectively.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">run_training_loop</span><span class="p">(</span><span class="n">train_dl</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_dl</span><span class="p">:</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">accelerator</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        
        <span class="c1"># convert target labels and predictions to string format for computing accuracy</span>
        <span class="n">preds</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">process_preds_and_labels</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">logits</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">])</span>
        <span class="c1"># add the target labels and predictions of this batch to seqeval</span>
        <span class="n">metric</span><span class="o">.</span><span class="n">add_batch</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">preds</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
</pre></div>
</div>
<p>Similarly, let’s write an evaluation loop as well.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">run_evaluation_loop</span><span class="p">(</span><span class="n">test_dl</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">test_dl</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">batch</span><span class="p">)</span>
            
            <span class="c1"># convert target labels and predictions to string format for computing accuracy</span>
            <span class="n">preds</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">process_preds_and_labels</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">logits</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">])</span>
            <span class="c1"># add the target labels and predictions of this batch to seqeval</span>
            <span class="n">metric</span><span class="o">.</span><span class="n">add_batch</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">preds</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
</pre></div>
</div>
<p>Now let’s use these functions and train our model for 3 epochs and save the model after each epoch.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">3</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">run_training_loop</span><span class="p">(</span><span class="n">train_dl</span><span class="p">)</span>
    <span class="c1"># compute training accuracy</span>
    <span class="n">train_acc</span> <span class="o">=</span> <span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">()[</span><span class="s1">&#39;overall_accuracy&#39;</span><span class="p">]</span>
    
    <span class="n">run_evaluation_loop</span><span class="p">(</span><span class="n">val_dl</span><span class="p">)</span>
    <span class="c1"># compute validation accuracy</span>
    <span class="n">val_acc</span> <span class="o">=</span> <span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">()[</span><span class="s1">&#39;overall_accuracy&#39;</span><span class="p">]</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2"> train_acc: </span><span class="si">{</span><span class="n">train_acc</span><span class="si">}</span><span class="s2"> val_acc: </span><span class="si">{</span><span class="n">val_acc</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="c1"># save the model at the end of epoch</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="sa">f</span><span class="s2">&quot;model-v</span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">.pt&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>After training for 3 epochs, these are our final results:</p>
<p><img alt="fastprogress_ner" src="_images/fastprogress_ner.png" /></p>
<p>If you are wondering how I got that nice looking table of results, <a class="reference external" href="https://github.com/fastai/fastprogress">fastprogress</a> is the answer for that. It’s similar to the <a class="reference external" href="https://github.com/tqdm/tqdm">tqdm</a> progress bar but I love the nicely formatted output given by fastprogress which drags me to use it. Especially, the print messages becomes a mess while training for longer number of epochs on notebooks, but fastprogress solves that for me :)</p>
<p>We get a similar overall accuracy on the test set as well. The below code returns all the metrics(including overall accuracy) on the test set:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">run_evaluation_loop</span><span class="p">(</span><span class="n">test_dl</span><span class="p">)</span>
<span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
</pre></div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="index.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Introduction</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="cv.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Image segmentation</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Bipin Krishnan P<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>
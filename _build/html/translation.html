
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Machine translation</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://github.com/translation.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Image segmentation" href="cv.html" />
    <link rel="prev" title="Masked language modelling" href="mlm.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="index.html">
   About this book
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  NLP
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="ner.html">
   Named entity recognition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="mlm.html">
   Masked language modelling
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Machine translation
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Computer vision
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="cv.html">
   Image segmentation
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/translation.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/bipinkrishnan/ml-powered-apps"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#preparing-the-dataset">
   Preparing the dataset
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#downloading-the-dataset">
     Downloading the dataset
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#preprocessing-the-dataset">
     Preprocessing the dataset
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#preparing-the-dataloader">
     Preparing the dataloader
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <section id="machine-translation">
<h1>Machine translation<a class="headerlink" href="#machine-translation" title="Permalink to this headline">¶</a></h1>
<p>Unlike other chapters that we’ve completed so far, this will be a bit more familiar to all of you guys. We don’t need a separate section to discuss “What is machine translation?”. It’s as simple as saying that given an english sentence, our machine learning model translates it to another language, say, Spanish.</p>
<p>In the above example, the inputs to our model will be an english sentence and the label will be it’s corresponding Spanish sentence.</p>
<p>Let’s directly jump into the datasets that we are going to use.</p>
<section id="preparing-the-dataset">
<h2>Preparing the dataset<a class="headerlink" href="#preparing-the-dataset" title="Permalink to this headline">¶</a></h2>
<section id="downloading-the-dataset">
<h3>Downloading the dataset<a class="headerlink" href="#downloading-the-dataset" title="Permalink to this headline">¶</a></h3>
<p>We will be using the <a class="reference external" href="https://huggingface.co/datasets/news_commentary">news commentary dataset</a> for our task, and specifically we will be using the english to french translation subset.</p>
<p>We will retrieve the dataset by specifying the languages we require for our task(that is, english and french).</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="n">raw_datasets</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;news_commentary&quot;</span><span class="p">,</span> <span class="n">lang1</span><span class="o">=</span><span class="s2">&quot;en&quot;</span><span class="p">,</span> <span class="n">lang2</span><span class="o">=</span><span class="s2">&quot;fr&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">raw_datasets</span><span class="p">)</span>
</pre></div>
</div>
<p>From the dataset, we will use 50% for training and 10% for evaluation purpose.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">split_datasets</span> <span class="o">=</span> <span class="n">raw_datasets</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span><span class="n">train_size</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">split_datasets</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">DatasetDict</span><span class="p">({</span>
    <span class="n">train</span><span class="p">:</span> <span class="n">Dataset</span><span class="p">({</span>
        <span class="n">features</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">,</span> <span class="s1">&#39;translation&#39;</span><span class="p">],</span>
        <span class="n">num_rows</span><span class="p">:</span> <span class="mi">104739</span>
    <span class="p">})</span>
    <span class="n">test</span><span class="p">:</span> <span class="n">Dataset</span><span class="p">({</span>
        <span class="n">features</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">,</span> <span class="s1">&#39;translation&#39;</span><span class="p">],</span>
        <span class="n">num_rows</span><span class="p">:</span> <span class="mi">20948</span>
    <span class="p">})</span>
<span class="p">})</span>
</pre></div>
</div>
</section>
<section id="preprocessing-the-dataset">
<h3>Preprocessing the dataset<a class="headerlink" href="#preprocessing-the-dataset" title="Permalink to this headline">¶</a></h3>
<p>The model we are going to use is already trained for translating english to french, we will finetune it for our news commentary dataset.</p>
<p>Since our inputs and labels are sentences, we need to tokenize both of them before using for training.</p>
<p>We will load the tokenizer of our model as we did in other chapters:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="c1"># model name</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="s2">&quot;Helsinki-NLP/opus-mt-en-fr&quot;</span>
<span class="c1"># load the tokenizer for the model</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</pre></div>
</div>
<p>Let’s take a sample containing english and french sentences from the training set for quick experiments:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sample</span> <span class="o">=</span> <span class="n">split_datasets</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">][</span><span class="s1">&#39;translation&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
   <span class="s1">&#39;en&#39;</span><span class="p">:</span> <span class="s1">&#39;It is important to note that these were unintended consequences of basically sensible policy decisions.&#39;</span><span class="p">,</span> 
   <span class="s1">&#39;fr&#39;</span><span class="p">:</span> <span class="s1">&#39;Il est important de noter qu’il s’agit là de conséquences non voulues de décisions politiques raisonnables au départ.&#39;</span>
<span class="p">}</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">'en'</span></code>(english) part will be the inputs and <code class="docutils literal notranslate"><span class="pre">'fr'</span></code>(french) part will be the labels for our model.</p>
<p>Let’s tokenize our inputs,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;en&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p>Since our model is already trained for english to french translation, tokenizing the input english sentence is simple as that. But for our french sentences, we need to let the tokenizer know that we are passing the labels inorder to get the correct tokenized output as shown below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">with</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">as_target_tokenizer</span><span class="p">():</span>
    <span class="n">french_tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">sample</span><span class="p">[</span><span class="s1">&#39;fr&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p>If we use the tokenizer without specifying anything, it will tokenize the french sentence as if it were an english sentence.</p>
<p>Now let’s wrap this inside a function and apply it to all the english-french sentences in our dataset. Apart from that we will truncate our sentences to a maximum length of 128:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">max_length</span> <span class="o">=</span> <span class="mi">128</span>

<span class="k">def</span> <span class="nf">tokenize</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
    <span class="n">en_sentences</span> <span class="o">=</span> <span class="p">[</span><span class="n">sent</span><span class="p">[</span><span class="s1">&#39;en&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">[</span><span class="s1">&#39;translation&#39;</span><span class="p">]]</span>
    <span class="n">fr_sentences</span> <span class="o">=</span> <span class="p">[</span><span class="n">sent</span><span class="p">[</span><span class="s1">&#39;fr&#39;</span><span class="p">]</span> <span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">examples</span><span class="p">[</span><span class="s1">&#39;translation&#39;</span><span class="p">]]</span>

    <span class="c1"># tokenize english sentences</span>
    <span class="n">model_inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">en_sentences</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># tokenize french sentences</span>
    <span class="k">with</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">as_target_tokenizer</span><span class="p">():</span>
        <span class="n">labels</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">fr_sentences</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># add tokenized french sentences as labels</span>
    <span class="n">model_inputs</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">model_inputs</span>
</pre></div>
</div>
<p>Let’s apply the function to our train and test set:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tokenized_datasets</span> <span class="o">=</span> <span class="n">split_datasets</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
    <span class="n">tokenize</span><span class="p">,</span> 
    <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
    <span class="n">remove_columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;id&#39;</span><span class="p">,</span> <span class="s1">&#39;translation&#39;</span><span class="p">]</span>
    <span class="p">)</span>
</pre></div>
</div>
</section>
<section id="preparing-the-dataloader">
<h3>Preparing the dataloader<a class="headerlink" href="#preparing-the-dataloader" title="Permalink to this headline">¶</a></h3>
<p>Since this is a sequence to sequence task, we will be using <code class="docutils literal notranslate"><span class="pre">DataCollatorForSeq2Seq</span></code> as our collator, which requires both the tokenizer and the model, so we will load our pretrained model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSeq2SeqLM</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSeq2SeqLM</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</pre></div>
</div>
<p>Now let’s define our collator:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DataCollatorForSeq2Seq</span>

<span class="n">collate_fn</span> <span class="o">=</span> <span class="n">DataCollatorForSeq2Seq</span><span class="p">(</span><span class="n">tokenizer</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">model</span><span class="p">)</span>
</pre></div>
</div>
<p>We used this collator specifically because it adds some extra information inside the dataloader, specific to sequence to sequence tasks, like, <code class="docutils literal notranslate"><span class="pre">decoder_input_ids</span></code> which is used by the decoder part of the model during training.</p>
<p>Let’s create our training and testing dataloader:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>
<span class="c1"># training dataloader</span>
<span class="n">train_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">tokenized_datasets</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">],</span> 
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> 
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
    <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_fn</span>
<span class="p">)</span>

<span class="c1"># test dataloader</span>
<span class="n">test_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">tokenized_datasets</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">],</span> 
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> 
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> 
    <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_fn</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Let’s see what information present inside the dataloaders:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">batch</span> <span class="o">=</span> <span class="nb">next</span><span class="p">(</span><span class="nb">iter</span><span class="p">(</span><span class="n">train_dl</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="n">batch</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dict_keys</span><span class="p">([</span><span class="s1">&#39;input_ids&#39;</span><span class="p">,</span> <span class="s1">&#39;attention_mask&#39;</span><span class="p">,</span> <span class="s1">&#39;labels&#39;</span><span class="p">,</span> <span class="s1">&#39;decoder_input_ids&#39;</span><span class="p">])</span>
</pre></div>
</div>
<p>As you can see that, apart from <code class="docutils literal notranslate"><span class="pre">input_ids</span></code>, <code class="docutils literal notranslate"><span class="pre">attention_mask</span></code> and <code class="docutils literal notranslate"><span class="pre">labels</span></code>, we’ve one more key called <code class="docutils literal notranslate"><span class="pre">decoder_input_ids</span></code> which is added by our collator.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="mlm.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Masked language modelling</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="cv.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Image segmentation</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Bipin Krishnan P<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q datasets accelerate sacrebleu wandb","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-05T08:39:17.230533Z","iopub.execute_input":"2022-03-05T08:39:17.230887Z","iopub.status.idle":"2022-03-05T08:39:26.700335Z","shell.execute_reply.started":"2022-03-05T08:39:17.230847Z","shell.execute_reply":"2022-03-05T08:39:26.699205Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset, load_metric\nfrom transformers import AutoTokenizer, DataCollatorForSeq2Seq, AutoModelForSeq2SeqLM\n\nimport torch\nfrom torch import optim\nfrom torch.utils.data import DataLoader\n\nfrom accelerate import Accelerator\nfrom tqdm.auto import tqdm\nimport wandb","metadata":{"execution":{"iopub.status.busy":"2022-03-05T08:39:26.703032Z","iopub.execute_input":"2022-03-05T08:39:26.703383Z","iopub.status.idle":"2022-03-05T08:39:27.389799Z","shell.execute_reply.started":"2022-03-05T08:39:26.703341Z","shell.execute_reply":"2022-03-05T08:39:27.388299Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"raw_datasets = load_dataset(\"news_commentary\", lang1=\"en\", lang2=\"fr\")\nprint(raw_datasets)","metadata":{"execution":{"iopub.status.busy":"2022-03-05T08:37:21.032768Z","iopub.execute_input":"2022-03-05T08:37:21.033625Z","iopub.status.idle":"2022-03-05T08:37:45.516123Z","shell.execute_reply.started":"2022-03-05T08:37:21.033580Z","shell.execute_reply":"2022-03-05T08:37:45.514955Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"split_datasets = raw_datasets['train'].train_test_split(train_size=0.5, test_size=0.1, seed=42)\nprint(split_datasets)","metadata":{"execution":{"iopub.status.busy":"2022-03-05T08:37:45.518298Z","iopub.execute_input":"2022-03-05T08:37:45.518566Z","iopub.status.idle":"2022-03-05T08:37:45.565949Z","shell.execute_reply.started":"2022-03-05T08:37:45.518536Z","shell.execute_reply":"2022-03-05T08:37:45.564826Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# model name\ncheckpoint = \"Helsinki-NLP/opus-mt-en-fr\"\n# load the tokenizer for the model\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)","metadata":{"execution":{"iopub.status.busy":"2022-03-05T08:37:45.568260Z","iopub.execute_input":"2022-03-05T08:37:45.568639Z","iopub.status.idle":"2022-03-05T08:38:06.683400Z","shell.execute_reply.started":"2022-03-05T08:37:45.568591Z","shell.execute_reply":"2022-03-05T08:38:06.682109Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"sample = split_datasets['train']['translation'][0]\nprint(sample)","metadata":{"execution":{"iopub.status.busy":"2022-03-05T08:38:06.685148Z","iopub.execute_input":"2022-03-05T08:38:06.685568Z","iopub.status.idle":"2022-03-05T08:38:09.269231Z","shell.execute_reply.started":"2022-03-05T08:38:06.685518Z","shell.execute_reply":"2022-03-05T08:38:09.268006Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"tokenizer(sample['en'])","metadata":{"execution":{"iopub.status.busy":"2022-03-05T08:38:09.271020Z","iopub.execute_input":"2022-03-05T08:38:09.271343Z","iopub.status.idle":"2022-03-05T08:38:09.280483Z","shell.execute_reply.started":"2022-03-05T08:38:09.271307Z","shell.execute_reply":"2022-03-05T08:38:09.279604Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"max_length = 128\n\ndef tokenize(examples):\n    en_sentences = [sent['en'] for sent in examples['translation']]\n    fr_sentences = [sent['fr'] for sent in examples['translation']]\n\n    # tokenize english sentences\n    model_inputs = tokenizer(en_sentences, max_length=max_length, truncation=True)\n\n    # tokenize french sentences\n    with tokenizer.as_target_tokenizer():\n        labels = tokenizer(fr_sentences, max_length=max_length, truncation=True)\n\n    # add tokenized french sentences as labels\n    model_inputs['labels'] = labels['input_ids']\n    return model_inputs","metadata":{"execution":{"iopub.status.busy":"2022-03-05T08:38:09.281964Z","iopub.execute_input":"2022-03-05T08:38:09.282461Z","iopub.status.idle":"2022-03-05T08:38:09.301001Z","shell.execute_reply.started":"2022-03-05T08:38:09.282424Z","shell.execute_reply":"2022-03-05T08:38:09.299468Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"tokenized_datasets = split_datasets.map(tokenize, batched=True, remove_columns=['id', 'translation'])","metadata":{"execution":{"iopub.status.busy":"2022-03-05T08:28:29.007938Z","iopub.execute_input":"2022-03-05T08:28:29.008490Z","iopub.status.idle":"2022-03-05T08:29:29.036596Z","shell.execute_reply.started":"2022-03-05T08:28:29.008450Z","shell.execute_reply":"2022-03-05T08:29:29.035911Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"batch_size = 32\n\nmodel = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\ncollate_fn = DataCollatorForSeq2Seq(tokenizer, model=model)\n\ntrain_dl = DataLoader(\n    tokenized_datasets['train'], \n    batch_size=batch_size, \n    shuffle=True, \n    collate_fn=collate_fn\n)\n\ntest_dl = DataLoader(\n    tokenized_datasets['test'], \n    batch_size=batch_size, \n    shuffle=False, \n    collate_fn=collate_fn\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-05T08:38:09.302797Z","iopub.execute_input":"2022-03-05T08:38:09.303360Z","iopub.status.idle":"2022-03-05T08:38:36.565207Z","shell.execute_reply.started":"2022-03-05T08:38:09.303319Z","shell.execute_reply":"2022-03-05T08:38:36.564344Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"batch = next(iter(train_dl))\n\nprint(batch.keys())","metadata":{"execution":{"iopub.status.busy":"2022-03-05T08:29:41.429037Z","iopub.execute_input":"2022-03-05T08:29:41.429267Z","iopub.status.idle":"2022-03-05T08:29:41.466576Z","shell.execute_reply.started":"2022-03-05T08:29:41.429239Z","shell.execute_reply":"2022-03-05T08:29:41.465366Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"opt = optim.AdamW(model.parameters(), lr=5.34e-6)\n\naccelerator = Accelerator()\ntrain_dl, test_dl, model, opt = accelerator.prepare(\n    train_dl, test_dl, model, opt\n)","metadata":{"execution":{"iopub.status.busy":"2022-03-05T08:29:41.467900Z","iopub.execute_input":"2022-03-05T08:29:41.468157Z","iopub.status.idle":"2022-03-05T08:29:45.862812Z","shell.execute_reply.started":"2022-03-05T08:29:41.468124Z","shell.execute_reply":"2022-03-05T08:29:45.861810Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"metric = load_metric('sacrebleu')","metadata":{"execution":{"iopub.status.busy":"2022-03-05T08:29:45.864090Z","iopub.execute_input":"2022-03-05T08:29:45.864350Z","iopub.status.idle":"2022-03-05T08:29:46.640299Z","shell.execute_reply.started":"2022-03-05T08:29:45.864315Z","shell.execute_reply":"2022-03-05T08:29:46.639636Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"prediction = ['So it can happen anywhere.']\nlabel = ['So it is happen anywhere.']\n\nmetric.compute(predictions=prediction, references=[label])","metadata":{"execution":{"iopub.status.busy":"2022-03-05T08:29:46.641452Z","iopub.execute_input":"2022-03-05T08:29:46.641763Z","iopub.status.idle":"2022-03-05T08:29:46.656069Z","shell.execute_reply.started":"2022-03-05T08:29:46.641724Z","shell.execute_reply":"2022-03-05T08:29:46.655290Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def process_preds_and_labels(preds, labels):\n    preds = preds.detach().cpu()\n    labels = labels.detach().cpu()\n    # replace all -100 with the token id of <pad>\n    labels = torch.where(labels==-100, tokenizer.pad_token_id, labels)\n    \n    # decode all token ids to its string/text format\n    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n    \n    # additional cleaning by removing begining and trailing spaces\n    decoded_preds = [pred.strip() for pred in decoded_preds]\n    decoded_labels = [[label.strip()] for label in decoded_labels]\n    \n    return decoded_preds, decoded_labels","metadata":{"execution":{"iopub.status.busy":"2022-03-05T08:29:46.657526Z","iopub.execute_input":"2022-03-05T08:29:46.658129Z","iopub.status.idle":"2022-03-05T08:29:46.664737Z","shell.execute_reply.started":"2022-03-05T08:29:46.658093Z","shell.execute_reply":"2022-03-05T08:29:46.664051Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"def run_training(train_dl):\n    model.train()\n    for batch in tqdm(train_dl, total=len(train_dl)):\n        opt.zero_grad()\n        out = model(**batch)\n        accelerator.backward(out.loss)\n        opt.step()\n        \ndef run_evaluation(test_dl):\n    model.eval()\n    with torch.no_grad():\n        for batch in tqdm(test_dl, total=len(test_dl)):\n            # generate predictions one by one\n            preds = model.generate(\n                input_ids=batch['input_ids'],\n                attention_mask=batch['attention_mask'],\n                max_length=max_length,\n            )\n            \n            # convert target labels and predictions to string format for computing accuracy\n            preds, labels = process_preds_and_labels(preds, batch['labels'])\n            # add the target labels and predictions of this batch to seqeval\n            metric.add_batch(predictions=preds, references=labels)","metadata":{"execution":{"iopub.status.busy":"2022-03-05T08:29:46.666095Z","iopub.execute_input":"2022-03-05T08:29:46.666749Z","iopub.status.idle":"2022-03-05T08:29:46.675128Z","shell.execute_reply.started":"2022-03-05T08:29:46.666713Z","shell.execute_reply":"2022-03-05T08:29:46.674415Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"epochs = 3\n\nfor epoch in range(epochs):\n    run_training(train_dl)\n    \n    run_evaluation(test_dl)\n    # calculate BLEU score on test set\n    test_acc = metric.compute()['score']\n    \n    # save the model at the end of epoch\n    torch.save(model.state_dict(), f\"model-v{epoch}.pt\")\n    print(f\"epoch: {epoch} test_acc: {test_acc}\")","metadata":{"execution":{"iopub.status.busy":"2022-03-05T08:31:37.349572Z","iopub.execute_input":"2022-03-05T08:31:37.350248Z","iopub.status.idle":"2022-03-05T08:31:45.427326Z","shell.execute_reply.started":"2022-03-05T08:31:37.350212Z","shell.execute_reply":"2022-03-05T08:31:45.426539Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"### Testing","metadata":{}},{"cell_type":"code","source":"import wandb\n\nrun = wandb.init()\nartifact = run.use_artifact('bipin/machine-translation/model:v4', type='model')\nartifact_dir = artifact.download()\n\nmodel_file = \"./artifacts/model:v4/model-v2.pt\"\nmodel = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)\nmodel.load_state_dict(torch.load(model_file, map_location='cpu'))\nmodel.eval()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"split_datasets['test'][0]['translation']","metadata":{"execution":{"iopub.status.busy":"2022-03-05T08:41:31.289624Z","iopub.execute_input":"2022-03-05T08:41:31.289927Z","iopub.status.idle":"2022-03-05T08:41:31.299837Z","shell.execute_reply.started":"2022-03-05T08:41:31.289898Z","shell.execute_reply":"2022-03-05T08:41:31.298847Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"sample = split_datasets['test'][0]['translation']\ninputs = sample['en']\nlabel = sample['fr']\n\ninputs = tokenizer(inputs, return_tensors='pt')\nout = model.generate(**inputs)\n\n# convert token ids to string\nwith tokenizer.as_target_tokenizer():\n    decoded_out = tokenizer.decode(out[0], skip_special_tokens=True)\n\nprint(\"Label: \\n\", label)\nprint(\"Prediction: \\n\", decoded_out)","metadata":{"execution":{"iopub.status.busy":"2022-03-05T08:43:46.955649Z","iopub.execute_input":"2022-03-05T08:43:46.955992Z","iopub.status.idle":"2022-03-05T08:43:47.643204Z","shell.execute_reply.started":"2022-03-05T08:43:46.955959Z","shell.execute_reply":"2022-03-05T08:43:47.642240Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
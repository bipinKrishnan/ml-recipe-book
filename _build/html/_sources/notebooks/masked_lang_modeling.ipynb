{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q datasets accelerate","metadata":{"execution":{"iopub.status.busy":"2022-02-26T05:39:02.268482Z","iopub.execute_input":"2022-02-26T05:39:02.268750Z","iopub.status.idle":"2022-02-26T05:39:09.704587Z","shell.execute_reply.started":"2022-02-26T05:39:02.268716Z","shell.execute_reply":"2022-02-26T05:39:09.703678Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"from datasets import load_dataset\nfrom transformers import (\n    AutoTokenizer, \n    DataCollatorForLanguageModeling,\n    default_data_collator,\n    AutoModelForMaskedLM,\n)\n\nfrom accelerate import Accelerator\nimport math\nfrom tqdm.auto import tqdm\n\nimport torch\nfrom torch import optim\nfrom torch.utils.data import DataLoader","metadata":{"execution":{"iopub.status.busy":"2022-02-26T05:39:09.707285Z","iopub.execute_input":"2022-02-26T05:39:09.707617Z","iopub.status.idle":"2022-02-26T05:39:09.714239Z","shell.execute_reply.started":"2022-02-26T05:39:09.707561Z","shell.execute_reply":"2022-02-26T05:39:09.713519Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"raw_datasets = load_dataset('xsum')\nraw_datasets","metadata":{"execution":{"iopub.status.busy":"2022-02-26T05:40:21.130447Z","iopub.execute_input":"2022-02-26T05:40:21.130739Z","iopub.status.idle":"2022-02-26T05:42:02.017208Z","shell.execute_reply.started":"2022-02-26T05:40:21.130707Z","shell.execute_reply":"2022-02-26T05:42:02.016507Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_size = 10000\ntest_size = int(0.2*train_size)\nseed = 42\n\n# 10,000 rows for training and 2000 rows for testing\ndownsampled_datasets = raw_datasets['train'].train_test_split(\n    train_size=train_size,\n    test_size=test_size,\n    seed=seed,\n)\n\nprint(downsampled_datasets)","metadata":{"execution":{"iopub.status.busy":"2022-02-26T05:44:30.446432Z","iopub.execute_input":"2022-02-26T05:44:30.446748Z","iopub.status.idle":"2022-02-26T05:44:30.474968Z","shell.execute_reply.started":"2022-02-26T05:44:30.446702Z","shell.execute_reply":"2022-02-26T05:44:30.474268Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"checkpoint = 'distilbert-base-uncased'\n\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)","metadata":{"execution":{"iopub.status.busy":"2022-02-26T05:44:30.704070Z","iopub.execute_input":"2022-02-26T05:44:30.704322Z","iopub.status.idle":"2022-02-26T05:44:36.479397Z","shell.execute_reply.started":"2022-02-26T05:44:30.704295Z","shell.execute_reply":"2022-02-26T05:44:36.478663Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"chunk_size = 128\n\ndef create_chunks(examples):\n    # tokenize the inputs\n    inputs = tokenizer(examples['document'])\n    # cocatenate the inputs\n    concatenated_examples = {k: sum(v, []) for k, v in inputs.items()}\n    total_len = (len(concatenated_examples['input_ids'])//chunk_size)*chunk_size\n    \n    # create chunks of size 128\n    results = {\n        k: [v[i: (i+chunk_size)] for i in range(0, total_len, chunk_size)] \n        for k, v in concatenated_examples.items()\n        }\n    \n    results['labels'] = results['input_ids'].copy()\n    return results","metadata":{"execution":{"iopub.status.busy":"2022-02-26T05:44:37.442471Z","iopub.execute_input":"2022-02-26T05:44:37.443262Z","iopub.status.idle":"2022-02-26T05:44:37.449293Z","shell.execute_reply.started":"2022-02-26T05:44:37.443222Z","shell.execute_reply":"2022-02-26T05:44:37.448637Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"preprocessed_datasets = downsampled_datasets.map(\n    create_chunks, \n    batched=True, \n    remove_columns=['document', 'summary', 'id']\n)","metadata":{"execution":{"iopub.status.busy":"2022-02-26T05:44:39.102472Z","iopub.execute_input":"2022-02-26T05:44:39.103182Z","iopub.status.idle":"2022-02-26T05:45:18.707499Z","shell.execute_reply.started":"2022-02-26T05:44:39.103141Z","shell.execute_reply":"2022-02-26T05:45:18.706756Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"sample = preprocessed_datasets['train'][:5]\n\nfor i in sample['input_ids']:\n    input_length = len(i)\n    \n    print(input_length)","metadata":{"execution":{"iopub.status.busy":"2022-02-26T05:45:22.902192Z","iopub.execute_input":"2022-02-26T05:45:22.902851Z","iopub.status.idle":"2022-02-26T05:45:22.912048Z","shell.execute_reply.started":"2022-02-26T05:45:22.902806Z","shell.execute_reply":"2022-02-26T05:45:22.910653Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"sample_inputs = sample['input_ids'][0]\nsample_labels = sample['labels'][0]\n\n# decode the tokens\nprint(\"INPUTS:\\n\", tokenizer.decode(sample_inputs))\nprint(\"\\nLABELS:\\n\", tokenizer.decode(sample_labels))","metadata":{"execution":{"iopub.status.busy":"2022-02-26T05:45:24.159394Z","iopub.execute_input":"2022-02-26T05:45:24.159949Z","iopub.status.idle":"2022-02-26T05:45:25.000661Z","shell.execute_reply.started":"2022-02-26T05:45:24.159909Z","shell.execute_reply":"2022-02-26T05:45:24.999938Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"collate_fn = DataCollatorForLanguageModeling(\n    tokenizer, \n    mlm_probability=0.15\n    )","metadata":{"execution":{"iopub.status.busy":"2022-02-26T05:45:27.022120Z","iopub.execute_input":"2022-02-26T05:45:27.022588Z","iopub.status.idle":"2022-02-26T05:45:27.026062Z","shell.execute_reply.started":"2022-02-26T05:45:27.022549Z","shell.execute_reply":"2022-02-26T05:45:27.025353Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"for i in zip(sample):\n    print(i)","metadata":{"execution":{"iopub.status.busy":"2022-02-26T05:45:30.718399Z","iopub.execute_input":"2022-02-26T05:45:30.719045Z","iopub.status.idle":"2022-02-26T05:45:30.724096Z","shell.execute_reply.started":"2022-02-26T05:45:30.719005Z","shell.execute_reply":"2022-02-26T05:45:30.723254Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# first 5 examples from train set\nfirst_5_rows = preprocessed_datasets['train'][:5]\ninput_list = [dict(zip(first_5_rows, v)) for v in zip(*first_5_rows.values())]","metadata":{"execution":{"iopub.status.busy":"2022-02-26T05:45:32.141452Z","iopub.execute_input":"2022-02-26T05:45:32.142314Z","iopub.status.idle":"2022-02-26T05:45:32.149132Z","shell.execute_reply.started":"2022-02-26T05:45:32.142262Z","shell.execute_reply":"2022-02-26T05:45:32.148257Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"collate_fn(input_list)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 64\n\ntrain_dl = DataLoader(\n    preprocessed_datasets['train'], \n    batch_size=batch_size, \n    shuffle=True,\n    collate_fn=collate_fn\n    )","metadata":{"execution":{"iopub.status.busy":"2022-02-26T05:45:39.722541Z","iopub.execute_input":"2022-02-26T05:45:39.723293Z","iopub.status.idle":"2022-02-26T05:45:39.728181Z","shell.execute_reply.started":"2022-02-26T05:45:39.723254Z","shell.execute_reply":"2022-02-26T05:45:39.727449Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"next(iter(train_dl))['input_ids'].shape","metadata":{"execution":{"iopub.status.busy":"2022-02-26T05:45:41.732887Z","iopub.execute_input":"2022-02-26T05:45:41.733495Z","iopub.status.idle":"2022-02-26T05:45:41.776372Z","shell.execute_reply.started":"2022-02-26T05:45:41.733453Z","shell.execute_reply":"2022-02-26T05:45:41.775679Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def apply_random_mask(examples):\n    example_list = [dict(zip(examples, v)) for v in zip(*examples.values())]\n    output = collate_fn(example_list)\n    # we need to return a dictionary\n    return {k: v.numpy() for k, v in output.items()}\n\ntest_dataset = preprocessed_datasets['test'].map(apply_random_mask, batched=True)","metadata":{"execution":{"iopub.status.busy":"2022-02-26T05:45:44.029492Z","iopub.execute_input":"2022-02-26T05:45:44.030216Z","iopub.status.idle":"2022-02-26T05:45:46.456377Z","shell.execute_reply.started":"2022-02-26T05:45:44.030178Z","shell.execute_reply":"2022-02-26T05:45:46.455664Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"test_dl = DataLoader(\n    test_dataset, \n    batch_size=batch_size, \n    shuffle=False, \n    collate_fn=default_data_collator\n    )","metadata":{"execution":{"iopub.status.busy":"2022-02-26T05:45:46.457916Z","iopub.execute_input":"2022-02-26T05:45:46.458255Z","iopub.status.idle":"2022-02-26T05:45:46.464202Z","shell.execute_reply.started":"2022-02-26T05:45:46.458216Z","shell.execute_reply":"2022-02-26T05:45:46.463560Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"tokenizer.decode(next(iter(test_dl))['input_ids'][0])","metadata":{"execution":{"iopub.status.busy":"2022-02-26T05:45:48.501550Z","iopub.execute_input":"2022-02-26T05:45:48.501840Z","iopub.status.idle":"2022-02-26T05:45:48.533948Z","shell.execute_reply.started":"2022-02-26T05:45:48.501810Z","shell.execute_reply":"2022-02-26T05:45:48.533302Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForMaskedLM.from_pretrained(checkpoint)\nopt = optim.AdamW(model.parameters(), lr=1.23e-5)\n\naccelerator = Accelerator()\ntrain_dl, test_dl, model, opt = accelerator.prepare(train_dl, test_dl, model, opt)","metadata":{"execution":{"iopub.status.busy":"2022-02-26T05:49:36.041126Z","iopub.execute_input":"2022-02-26T05:49:36.041902Z","iopub.status.idle":"2022-02-26T05:49:38.295646Z","shell.execute_reply.started":"2022-02-26T05:49:36.041849Z","shell.execute_reply":"2022-02-26T05:49:38.294751Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"def run_training_loop(train_dl):\n    losses = 0\n    model.train()\n    for batch in tqdm(train_dl, total=len(train_dl)):\n        opt.zero_grad()\n        out = model(**batch)\n        accelerator.backward(out.loss)\n        opt.step()\n\n        losses += out.loss.item()\n#         break\n    losses /= len(train_dl)\n    # exponential of cross entropy\n    perplexity = math.exp(losses)\n    return perplexity\n\ndef run_evaluation_loop(test_dl):\n    losses = 0\n    model.eval()\n    \n    with torch.no_grad():\n        for batch in tqdm(test_dl, total=len(test_dl)):\n            out = model(**batch)\n            losses += out.loss.item()\n#             break\n            \n    losses /= len(test_dl)\n    # exponential of cross entropy\n    perplexity = math.exp(losses)\n    return perplexity","metadata":{"execution":{"iopub.status.busy":"2022-02-26T05:49:41.464054Z","iopub.execute_input":"2022-02-26T05:49:41.464310Z","iopub.status.idle":"2022-02-26T05:49:41.472783Z","shell.execute_reply.started":"2022-02-26T05:49:41.464282Z","shell.execute_reply":"2022-02-26T05:49:41.471747Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"code","source":"epochs = 3\n\nfor epoch in range(epochs):\n    train_perplexity = run_training_loop(train_dl)\n    test_perplexity = run_evaluation_loop(test_dl)\n    \n    print(f\"epoch: {epoch} train_acc: {train_perplexity} val_acc: {test_perplexity}\")\n    \n    # save the model at the end of epoch\n    torch.save(model.state_dict(), f\"model-v{epoch}.pt\")","metadata":{"execution":{"iopub.status.busy":"2022-02-26T05:49:42.808473Z","iopub.execute_input":"2022-02-26T05:49:42.809187Z","iopub.status.idle":"2022-02-26T06:07:10.187574Z","shell.execute_reply.started":"2022-02-26T05:49:42.809148Z","shell.execute_reply":"2022-02-26T06:07:10.186687Z"},"trusted":true},"execution_count":30,"outputs":[]},{"cell_type":"markdown","source":"### Test example","metadata":{}},{"cell_type":"code","source":"text = \"\"\"\nRajesh Shah, one of the shop's co-owners, told the [MASK] \nthere would be a new name.\n\"\"\"\n\n# tokenize the inputs\ninputs = tokenizer(text, return_tensors='pt')\ninputs = inputs.to(accelerator.device)\nout = model(**inputs)\n\n# find the position in input where [MASK] is present\nmask_token_id = tokenizer.mask_token_id\nmask_idx = torch.where(inputs['input_ids']==mask_token_id)[1]\n\n# decode the prediction corresponding to [MASK]\npreds = out.logits.argmax(dim=-1)[0]\nmask_pred = tokenizer.decode(preds[mask_idx])\n\n# replace [MASK] with predicted word\nfinal_text = text.replace('[MASK]', mask_pred)\nprint(final_text)","metadata":{"execution":{"iopub.status.busy":"2022-02-26T06:09:43.269236Z","iopub.execute_input":"2022-02-26T06:09:43.269691Z","iopub.status.idle":"2022-02-26T06:09:43.288797Z","shell.execute_reply.started":"2022-02-26T06:09:43.269651Z","shell.execute_reply":"2022-02-26T06:09:43.288074Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
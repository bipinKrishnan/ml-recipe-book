
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Named entity recognition</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://github.com/named_entity_recognition.html" />
    <link rel="author" title="About these documents" href="about.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Masked language modeling" href="masked_language_modeling.html" />
    <link rel="prev" title="About this book" href="about.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="about.html">
   About this book
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Natural language processing
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Named entity recognition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="masked_language_modeling.html">
   Masked language modeling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="translation.html">
   Machine translation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="summarization.html">
   Summarization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="causal_language_modeling.html">
   Causal language modeling
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Image &amp; Text
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="image_captioning.html">
   Image captioning
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Computer vision
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="image_classification.html">
   Image classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="image_segmentation.html">
   Image segmentation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="object_detection.html">
   Object detection
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/named_entity_recognition.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/bipinkrishnan/ml-powered-apps"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-named-entity-recognition">
   What is named entity recognition?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#dataset">
   Dataset
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#downloading-the-dataset">
     Downloading the dataset
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#understanding-model-inputs">
     Understanding model inputs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#understanding-model-outputs">
     Understanding model outputs
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#creating-the-dataloaders">
     Creating the dataloaders
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#training-the-model">
   Training the model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#building-the-demo">
   Building the demo
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <section id="named-entity-recognition">
<h1>Named entity recognition<a class="headerlink" href="#named-entity-recognition" title="Permalink to this headline">¶</a></h1>
<p>The plan for this chapter is to:</p>
<ol class="simple">
<li><p>Learn about named entity recognition(NER)</p></li>
<li><p>Train a transformer model for NER</p></li>
<li><p>Build a web app demo(using Gradio) around the trained model to convert a sentence from western context to Indian context.</p></li>
</ol>
<p>If you did not get the last point, here is an example to make it clear:</p>
<p>If we pass the sentence ‘I am going to Paris’ as input, the model will modify the sentence and give the output as ‘I am going to Mumbai’. As you can see, the model identified the word ‘Paris’ and coverted it to ‘Mumbai’ which is more familiar to Indians. The idea for this demo is inspired from this <a class="reference external" href="https://towardsdatascience.com/practical-ai-using-nlp-word-vectors-in-a-novel-way-to-solve-the-problem-of-localization-9de3e4fbf56f">blog post</a>.</p>
<p>Below you can see the final demo that we will be building in this chapter:</p>
<p><img alt="NER Gradio demo" src="_images/ner_gr_demo.png" /></p>
<section id="what-is-named-entity-recognition">
<h2>What is named entity recognition?<a class="headerlink" href="#what-is-named-entity-recognition" title="Permalink to this headline">¶</a></h2>
<p>At present, there are a wide variety of tasks that machine learning models are capable of doing, named entity recognition or NER is one them. In short the job of a model trained for this task is to identify all the named entities in a given sentence.</p>
<p>Here is a figure showing what the model is expected to do for NER:</p>
<img alt="Named entity recignition" class="bg-primary mb-1 align-center" src="_images/ner_process.png" />
<p>In the above figure, the model recognizes ‘Sarah’ as a person(PER) and ‘London’ as a location(LOC) entity. Since the other words do not belong to any category of entities, no labels are present in the output for those words.</p>
<p>Named entity recognition does not limit to identitfying a person, location or organization, it can also be used for identifying parts of speech of each word in a sentence. The general term used to represent these kind of tasks are called token classification.</p>
<p>As humans, we try to understand the contents in a book by reading it word by word. Similarly, before passing a sentence into our models, we split them into simpler tokens using something called a tokenizer.</p>
<p>The simplest tokenizer you can think of is splitting a sentence into words as shown below:</p>
<p>Input sentence: <code class="docutils literal notranslate"><span class="pre">This</span> <span class="pre">is</span> <span class="pre">looking</span> <span class="pre">good</span></code></p>
<p>Output tokens: <code class="docutils literal notranslate"><span class="pre">['This',</span> <span class="pre">'is',</span> <span class="pre">'looking',</span> <span class="pre">'good']</span></code></p>
<p>And here is a figure to illustrate the same:</p>
<p><img alt="Named entity recignition tokenizer" src="_images/ner_tokenizer.png" /></p>
<p>We now have a basic understanding of the task and the related terms that we will encounter in this chapter. Now let’s talk about the dataset we will be using.</p>
</section>
<section id="dataset">
<h2>Dataset<a class="headerlink" href="#dataset" title="Permalink to this headline">¶</a></h2>
<section id="downloading-the-dataset">
<h3>Downloading the dataset<a class="headerlink" href="#downloading-the-dataset" title="Permalink to this headline">¶</a></h3>
<p>We will be using the <a class="reference external" href="https://huggingface.co/datasets/conllpp">conllpp</a> dataset which can be directly downloaded using the huggingface datasets library using the code shown below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="n">raw_datasets</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s2">&quot;conllpp&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>On printing <code class="docutils literal notranslate"><span class="pre">raw_datasets</span></code>, we get the structure of our dataset as shown below:</p>
<p><img alt="ner_data_dict" src="_images/ner_data_dict.png" /></p>
<p>The dataset is already split into train, validation and test sets where each set has these features/columns: id, tokens, pos_tags, chunk_tags and ner_tags</p>
<p>Can you guess the features that we will be using here?</p>
<p>If you guessed it correctly, <code class="docutils literal notranslate"><span class="pre">tokens</span></code> and <code class="docutils literal notranslate"><span class="pre">ner_tags</span></code> are the only features we require for the present use case.</p>
<p>This is how the first 5 rows of the training dataset will look like if it were a pandas dataframe:</p>
<p><img alt="ner_data_frame" src="_images/ner_data_frame.png" /></p>
<p>Now lets talk about the columns that we will be using for our task. The <code class="docutils literal notranslate"><span class="pre">tokens</span></code> column will be our inputs to the model and <code class="docutils literal notranslate"><span class="pre">ner_tags</span></code> is what the model should predict.</p>
</section>
<section id="understanding-model-inputs">
<h3>Understanding model inputs<a class="headerlink" href="#understanding-model-inputs" title="Permalink to this headline">¶</a></h3>
<p>As you can see, the column <code class="docutils literal notranslate"><span class="pre">tokens</span></code> are not sentences, instead it is a list of words(which is a kind of tokenization as we discussed earlier), so from now on we can call this as pre-tokenized inputs. Even though it’s already split into words, we cannot directly feed it to our model. Our model uses something called <strong>sub-word tokenizer</strong>, which is nothing but splitting a word into multiple subwords.</p>
<p>Let’s take an example and make it clear:</p>
<p>Input sentence: <code class="docutils literal notranslate"><span class="pre">&quot;This</span> <span class="pre">is</span> <span class="pre">the</span> <span class="pre">pytorch</span> <span class="pre">code&quot;</span></code></p>
<p>For the above sentence, a simple tokenizer that splits a sentence into words will give the following output: <code class="docutils literal notranslate"><span class="pre">['This',</span> <span class="pre">'is',</span> <span class="pre">'the',</span> <span class="pre">'pytorch',</span> <span class="pre">'code']</span></code></p>
<p>But a sub-word tokenizer may split some words into even simpler sub-words, just like this:</p>
<p><code class="docutils literal notranslate"><span class="pre">['This',</span> <span class="pre">'is',</span> <span class="pre">'the',</span> <span class="pre">'p',</span> <span class="pre">'##yt',</span> <span class="pre">'##or',</span> <span class="pre">'##ch',</span> <span class="pre">'code']</span></code></p>
<p>As you can see, some words like pytorch which is not commonly seen in sentences are split into multiple sub-words. Also note that except for the starting token ‘p’, all the sub-words for the word ‘pytorch’ starts with a ‘##’.</p>
<p>The model that we will be using for this task is <code class="docutils literal notranslate"><span class="pre">bert-base-cased</span></code> which is a bert model that treats cased and uncased words differently. This specific model will be helpful for our task because when writing the name of a person or a location, we always start with an upper-case letter which will make the job of our model way more easier while identifying named entities.</p>
<p>Now let’s write some code to tokenize each row in the <code class="docutils literal notranslate"><span class="pre">tokens</span></code> column using the <code class="docutils literal notranslate"><span class="pre">AutoTokenizer</span></code> module in <code class="docutils literal notranslate"><span class="pre">transformers</span></code> library.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>  <span class="c1"># to tokenize the inputs</span>

<span class="n">checkpoint</span> <span class="o">=</span> <span class="s1">&#39;bert-base-cased&#39;</span>

<span class="c1"># load the pretrained tokenizer from our checkpoint</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>

<span class="c1"># first row of the training set</span>
<span class="n">train_row_1</span> <span class="o">=</span> <span class="n">raw_datasets</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p>Now let’s pass in the <code class="docutils literal notranslate"><span class="pre">tokens</span></code> column of the first row into our tokenizer. Since our <code class="docutils literal notranslate"><span class="pre">tokens</span></code> column contain pre-tokenized inputs, we need to set the argument <code class="docutils literal notranslate"><span class="pre">is_split_into_words</span></code> to <code class="docutils literal notranslate"><span class="pre">True</span></code> while calling the tokenizer.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">train_row_1</span><span class="p">[</span><span class="s1">&#39;tokens&#39;</span><span class="p">],</span> <span class="n">is_split_into_words</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>If you print the <code class="docutils literal notranslate"><span class="pre">inputs</span></code>, you can see that our tokenizer has tokenized the words(as shown below) in such a way that it contains all the information to feed into our transformer model.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
    <span class="s1">&#39;input_ids&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">101</span><span class="p">,</span> <span class="mi">7270</span><span class="p">,</span> <span class="mi">22961</span><span class="p">,</span> <span class="mi">1528</span><span class="p">,</span> <span class="mi">1840</span><span class="p">,</span> <span class="mi">1106</span><span class="p">,</span> <span class="mi">21423</span><span class="p">,</span> <span class="mi">1418</span><span class="p">,</span> <span class="mi">2495</span><span class="p">,</span> <span class="mi">12913</span><span class="p">,</span> <span class="mi">119</span><span class="p">,</span> <span class="mi">102</span><span class="p">],</span> <span class="s1">&#39;token_type_ids&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> 
    <span class="s1">&#39;attention_mask&#39;</span><span class="p">:</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="p">}</span>
</pre></div>
</div>
</section>
<section id="understanding-model-outputs">
<h3>Understanding model outputs<a class="headerlink" href="#understanding-model-outputs" title="Permalink to this headline">¶</a></h3>
<p>As we’ve said earlier, <code class="docutils literal notranslate"><span class="pre">ner_tags</span></code> is the column that we will use as labels/outputs for our model. Here is the first 5 rows of this column:</p>
<img alt="ner_tags" class="bg-primary mb-1 align-center" src="_images/ner_tags.png" />
<p>As you can see, it’s a list of numbers. Let’s see what does these numbers actually mean.</p>
<p>You will get all the information about each feature in the dataset using <code class="docutils literal notranslate"><span class="pre">.features</span></code> method. Once you filter out only the <code class="docutils literal notranslate"><span class="pre">ner_tags</span></code> feature from that, you will get complete information about it, as shown below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">raw_datasets</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">features</span><span class="p">[</span><span class="s1">&#39;ner_tags&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>And you will get an output like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Sequence</span><span class="p">(</span><span class="n">feature</span><span class="o">=</span><span class="n">ClassLabel</span><span class="p">(</span><span class="n">num_classes</span><span class="o">=</span><span class="mi">9</span><span class="p">,</span> <span class="n">names</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;O&#39;</span><span class="p">,</span> <span class="s1">&#39;B-PER&#39;</span><span class="p">,</span> <span class="s1">&#39;I-PER&#39;</span><span class="p">,</span> <span class="s1">&#39;B-ORG&#39;</span><span class="p">,</span> <span class="s1">&#39;I-ORG&#39;</span><span class="p">,</span> <span class="s1">&#39;B-LOC&#39;</span><span class="p">,</span> <span class="s1">&#39;I-LOC&#39;</span><span class="p">,</span> <span class="s1">&#39;B-MISC&#39;</span><span class="p">,</span> <span class="s1">&#39;I-MISC&#39;</span><span class="p">],</span> <span class="n">names_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="kc">None</span><span class="p">),</span> <span class="n">length</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
</pre></div>
</div>
<p>Now, it’s clear that there are 9 different classes/entities inside the <code class="docutils literal notranslate"><span class="pre">ner_tags</span></code> and the name of each label is present inside <code class="docutils literal notranslate"><span class="pre">names</span></code>.</p>
<ol class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">PER</span></code>, <code class="docutils literal notranslate"><span class="pre">ORG</span></code>, <code class="docutils literal notranslate"><span class="pre">LOC</span></code> and <code class="docutils literal notranslate"><span class="pre">MISC</span></code> represent person, organisation, location and miscellaneous entities respectively. The only one remaining is <code class="docutils literal notranslate"><span class="pre">'O'</span></code> which can be used to represent words that doesn’t belong to any entity.</p></li>
<li><p>You might have noticed the <code class="docutils literal notranslate"><span class="pre">B-</span></code> and <code class="docutils literal notranslate"><span class="pre">I-</span></code> prefixes for all entities except <code class="docutils literal notranslate"><span class="pre">'O'</span></code>. Those prefixes represent whether a word is at the begining or inside an entity. Let me give you an example and make it clear.</p></li>
</ol>
<p>If the input is like this: <code class="docutils literal notranslate"><span class="pre">'My</span> <span class="pre">name</span> <span class="pre">is</span> <span class="pre">Roy</span> <span class="pre">Lee</span> <span class="pre">and</span> <span class="pre">I</span> <span class="pre">am</span> <span class="pre">from</span> <span class="pre">New</span> <span class="pre">York'</span></code></p>
<p>The corresponding labels should be: <code class="docutils literal notranslate"><span class="pre">['O',</span> <span class="pre">'O',</span> <span class="pre">'O',</span> <span class="pre">'B-PER',</span> <span class="pre">'I-PER',</span> <span class="pre">'O',</span> <span class="pre">'O',</span> <span class="pre">'O',</span> <span class="pre">'O',</span> <span class="pre">'B-LOC',</span> <span class="pre">'I-LOC']</span></code> where each label corresponds to each word in the input sentence.</p>
<p>As you can see, the words ‘Roy’ and ‘New’ are the words at the begining of a person and a location, so they are given the labels <code class="docutils literal notranslate"><span class="pre">B-PER</span></code> and <code class="docutils literal notranslate"><span class="pre">B-LOC</span></code> respectively. Whereas, the words ‘Lee’ and ‘York’ are not at the begining but inside a person and a location entity, so they are given the label <code class="docutils literal notranslate"><span class="pre">I-PER</span></code> and <code class="docutils literal notranslate"><span class="pre">I-LOC</span></code> respectively. All the words that does not belong to any entity is mapped to <code class="docutils literal notranslate"><span class="pre">O</span></code> tag.</p>
<p>Finally to wrap up this part, we will create a dictionary containing the id to label mapping.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># store all the ner labels</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">raw_datasets</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">features</span><span class="p">[</span><span class="s1">&#39;ner_tags&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">feature</span><span class="o">.</span><span class="n">names</span>

<span class="c1"># number of classes/ner tags -&gt; [0, 1, 2, 3, 4, 5, 6, 7, 8]</span>
<span class="n">ids</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">))</span>

<span class="c1"># id to label mapping </span>
<span class="n">id2label</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">ids</span><span class="p">,</span> <span class="n">labels</span><span class="p">))</span>
</pre></div>
</div>
<p>Now we have an id to label mapping as shown below:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span> <span class="p">{</span> 
   <span class="mi">0</span><span class="p">:</span> <span class="s1">&#39;O&#39;</span><span class="p">,</span>
   <span class="mi">1</span><span class="p">:</span> <span class="s1">&#39;B-PER&#39;</span><span class="p">,</span>
   <span class="mi">2</span><span class="p">:</span> <span class="s1">&#39;I-PER&#39;</span><span class="p">,</span>
   <span class="mi">3</span><span class="p">:</span> <span class="s1">&#39;B-ORG&#39;</span><span class="p">,</span>
   <span class="mi">4</span><span class="p">:</span> <span class="s1">&#39;I-ORG&#39;</span><span class="p">,</span>
   <span class="mi">5</span><span class="p">:</span> <span class="s1">&#39;B-LOC&#39;</span><span class="p">,</span>
   <span class="mi">6</span><span class="p">:</span> <span class="s1">&#39;I-LOC&#39;</span><span class="p">,</span>
   <span class="mi">7</span><span class="p">:</span> <span class="s1">&#39;B-MISC&#39;</span><span class="p">,</span>
   <span class="mi">8</span><span class="p">:</span> <span class="s1">&#39;I-MISC&#39;</span> 
 <span class="p">}</span>
</pre></div>
</div>
<p>As we have an understanding of the inputs as well outputs/labels of the dataset that we are going to use, it’s time to do some preprocessing and create a train, validation and test dataloader.</p>
</section>
<section id="creating-the-dataloaders">
<h3>Creating the dataloaders<a class="headerlink" href="#creating-the-dataloaders" title="Permalink to this headline">¶</a></h3>
<p>We need to complete the following tasks before wrapping everything inside a dataloader:</p>
<ol class="simple">
<li><p>Tokenize the inputs(<code class="docutils literal notranslate"><span class="pre">tokens</span></code> column)</p></li>
<li><p>Align the tokens and labels</p></li>
</ol>
<p>The first point is strainght forward and you must have got it. It’s just tokenizing our inputs as we did earlier in this chapter. Let’s talk about the second part.</p>
<p>Here is an example to discuss what aligning tokens and labels mean.</p>
<p>These are the inputs and outputs from the first row of our dataset:</p>
<p>Inputs: <code class="docutils literal notranslate"><span class="pre">['EU',</span> <span class="pre">'rejects',</span> <span class="pre">'German',</span> <span class="pre">'call',</span> <span class="pre">'to',</span> <span class="pre">'boycott',</span> <span class="pre">'British',</span> <span class="pre">'lamb',</span> <span class="pre">'.']</span></code></p>
<p>Outputs: <code class="docutils literal notranslate"><span class="pre">[3,</span> <span class="pre">0,</span> <span class="pre">7,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">7,</span> <span class="pre">0,</span> <span class="pre">0]</span></code></p>
<p>Each word in the input list corresponds to each label in the outputs list, so, their lengths are the same. We need to tokenize and convert the input strings to integers as we’ve shown earlier in the chapter. Since the outputs are already available as integers, we need not have to bother about them for now.</p>
<p>Let’s tokenize the inputs and see how the tokens look.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">train_row_1</span><span class="p">[</span><span class="s1">&#39;tokens&#39;</span><span class="p">],</span> <span class="n">is_split_into_words</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">inputs</span><span class="o">.</span><span class="n">tokens</span><span class="p">())</span>
</pre></div>
</div>
<p>These are the output tokens:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="s1">&#39;[CLS]&#39;</span><span class="p">,</span> <span class="s1">&#39;EU&#39;</span><span class="p">,</span> <span class="s1">&#39;rejects&#39;</span><span class="p">,</span> <span class="s1">&#39;German&#39;</span><span class="p">,</span> <span class="s1">&#39;call&#39;</span><span class="p">,</span> <span class="s1">&#39;to&#39;</span><span class="p">,</span> <span class="s1">&#39;boycott&#39;</span><span class="p">,</span> <span class="s1">&#39;British&#39;</span><span class="p">,</span> <span class="s1">&#39;la&#39;</span><span class="p">,</span> <span class="s1">&#39;##mb&#39;</span><span class="p">,</span> <span class="s1">&#39;.&#39;</span><span class="p">,</span> <span class="s1">&#39;[SEP]&#39;</span><span class="p">]</span>
</pre></div>
</div>
<p>As you can see there are two special tokens <code class="docutils literal notranslate"><span class="pre">[CLS]</span></code> and <code class="docutils literal notranslate"><span class="pre">[SEP]</span></code> at the begining and end of the sentence, those are specific to the model that we are using. <code class="docutils literal notranslate"><span class="pre">[CLS]</span></code> is special token added to the start of an input sentence whereas <code class="docutils literal notranslate"><span class="pre">[SEP]</span></code> is used as a separator between sentences. Since we have only a single sentence <code class="docutils literal notranslate"><span class="pre">[SEP]</span></code> is present at the end of the sentence.</p>
<p>Another problem is that the word ‘lamb’ is split into ‘la’ and ‘##mb’ by the tokenizer. Now the length of our input tokens is 12 whereas the length of the labels is 9 because our tokenizer added a <code class="docutils literal notranslate"><span class="pre">[CLS]</span></code>, <code class="docutils literal notranslate"><span class="pre">[SEP]</span></code> and <code class="docutils literal notranslate"><span class="pre">##mb</span></code> to our inputs.</p>
<p>For training, each token should have a label assigned to it, so we need to align the tokens and labels in such a way that both their lengths are same.</p>
<p>Fortunately, even after tokenization and splitting words into multiple sub-words, we could get the word ids of each token using <code class="docutils literal notranslate"><span class="pre">.word_ids()</span></code> method. Here is an example showing the same.</p>
<p>For the inputs <code class="docutils literal notranslate"><span class="pre">['EU',</span> <span class="pre">'rejects',</span> <span class="pre">'German',</span> <span class="pre">'call',</span> <span class="pre">'to',</span> <span class="pre">'boycott',</span> <span class="pre">'British',</span> <span class="pre">'lamb',</span> <span class="pre">'.']</span></code>, the word ids will be the index of the word in the list.</p>
<p>So, if we tokenize the above inputs using the tokenizer and take the word ids, this is what we get: <code class="docutils literal notranslate"><span class="pre">[None,</span> <span class="pre">0,</span> <span class="pre">1,</span> <span class="pre">2,</span> <span class="pre">3,</span> <span class="pre">4,</span> <span class="pre">5,</span> <span class="pre">6,</span> <span class="pre">7,</span> <span class="pre">7,</span> <span class="pre">8,</span> <span class="pre">None]</span></code></p>
<p>The two <code class="docutils literal notranslate"><span class="pre">None</span></code> values at the start and end represents <code class="docutils literal notranslate"><span class="pre">[CLS]</span></code> and <code class="docutils literal notranslate"><span class="pre">[SEP]</span></code> tokens. The rest of the integers represent the word ids for each token. You can clearly see that the only word id that is repeating twice is 7 which is the word id for ‘lamb’. Since it’s split into ‘la’ and ‘##mb’, both of them have the same word ids.</p>
<p>Here is an illustrated diagram to make the above process clear:</p>
<img alt="ner_word_ids" class="bg-primary mb-1 align-center" src="_images/ner_word_ids.png" />
<p>Now let’s write a simple function to align labels with tokens.</p>
<p>The function will take a list of word ids and its corresponding ner labels as arguments and then the following things happen. We loop through each word id in the provided list and checks whether the word id is equal to <code class="docutils literal notranslate"><span class="pre">None</span></code>(special tokens), if so we assign it a label of -100, otherwise we assign the label corresponding to its word id.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">align_tokens_and_labels</span><span class="p">(</span><span class="n">word_ids</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="n">previous_word_id</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="n">new_labels</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">for</span> <span class="n">word_id</span> <span class="ow">in</span> <span class="n">word_ids</span><span class="p">:</span>
        
        <span class="k">if</span> <span class="n">word_id</span><span class="o">!=</span><span class="n">previous_word_id</span><span class="p">:</span>
            <span class="n">label</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span> <span class="k">if</span> <span class="n">word_id</span><span class="o">==</span><span class="kc">None</span> <span class="k">else</span> <span class="n">labels</span><span class="p">[</span><span class="n">word_id</span><span class="p">]</span>
        <span class="k">elif</span> <span class="n">word_id</span><span class="o">==</span><span class="kc">None</span><span class="p">:</span>
            <span class="n">label</span> <span class="o">=</span> <span class="o">-</span><span class="mi">100</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">label</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">word_id</span><span class="p">]</span>

            <span class="c1"># checks if the ner label is of the form B-XXX</span>
            <span class="k">if</span> <span class="n">label</span><span class="o">%</span><span class="mi">2</span><span class="o">==</span><span class="mi">1</span><span class="p">:</span>
                <span class="c1"># converts the label from B-XXX to I-XXX</span>
                <span class="n">label</span> <span class="o">+=</span> <span class="mi">1</span>
                
        <span class="n">previous_word_id</span> <span class="o">=</span> <span class="n">word_id</span>
        <span class="n">new_labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>
                
    <span class="k">return</span> <span class="n">new_labels</span>
</pre></div>
</div>
<p>Let’s test the function:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">ner_labels</span> <span class="o">=</span> <span class="n">train_row_1</span><span class="p">[</span><span class="s1">&#39;ner_tags&#39;</span><span class="p">]</span>

<span class="c1"># tokenized inputs</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">train_row_1</span><span class="p">[</span><span class="s1">&#39;tokens&#39;</span><span class="p">],</span> <span class="n">is_split_into_words</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">word_ids</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">word_ids</span><span class="p">()</span>

<span class="n">align_tokens_and_labels</span><span class="p">(</span><span class="n">word_ids</span><span class="p">,</span> <span class="n">ner_labels</span><span class="p">)</span>
</pre></div>
</div>
<p>Here are the output labels:
<code class="docutils literal notranslate"><span class="pre">[-100,</span> <span class="pre">3,</span> <span class="pre">0,</span> <span class="pre">7,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">7,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">0,</span> <span class="pre">-100]</span></code></p>
<p>Now we can write a single function to apply this to every example in our dataset. Our final function should take in a batch of examples from our dataset and loop through each example in the batch, align the tokens and labels and finally return the tokenized inputs and corresponding labels.</p>
<p>We also truncate our tokens to a length of 512. Any input example that has a length higher than this are shortened/truncated to 512. The default maximum length of our tokenizer is 512, so we just need to set <code class="docutils literal notranslate"><span class="pre">truncation=True</span></code> while tokenizing the inputs.</p>
<div class="admonition note">
<p class="admonition-title">Note</p>
<p>We can tokenize a group of input examples together and get the word ids using indexing.</p>
<p>For example, if you’ve tokenized 3 sentences together, you can get the word ids of the first sentence using <code class="docutils literal notranslate"><span class="pre">inputs.word_ids(0)</span></code>.</p>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">prepare_inputs_and_labels</span><span class="p">(</span><span class="n">ds</span><span class="p">):</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">ds</span><span class="p">[</span><span class="s1">&#39;tokens&#39;</span><span class="p">],</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">is_split_into_words</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">labels_batch</span> <span class="o">=</span> <span class="n">ds</span><span class="p">[</span><span class="s1">&#39;ner_tags&#39;</span><span class="p">]</span>
    
    <span class="n">new_labels</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="c1"># loop through each example in the batch</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">labels</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">labels_batch</span><span class="p">):</span>
        <span class="c1"># extract the word ids using the index</span>
        <span class="n">word_ids</span> <span class="o">=</span> <span class="n">inputs</span><span class="o">.</span><span class="n">word_ids</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
        <span class="n">new_label</span> <span class="o">=</span> <span class="n">align_tokens_and_labels</span><span class="p">(</span><span class="n">word_ids</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>
        <span class="n">new_labels</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">new_label</span><span class="p">)</span>
        
    <span class="n">inputs</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_labels</span>
    <span class="k">return</span> <span class="n">inputs</span>
</pre></div>
</div>
<p>Now let’s apply this function to our dataset as below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">prepared_datasets</span> <span class="o">=</span> <span class="n">raw_datasets</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
    <span class="n">prepare_inputs_and_labels</span><span class="p">,</span> 
    <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
    <span class="n">remove_columns</span><span class="o">=</span><span class="n">raw_datasets</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">column_names</span>
<span class="p">)</span>
</pre></div>
</div>
<p>We used the <code class="docutils literal notranslate"><span class="pre">.map</span></code> method and set <code class="docutils literal notranslate"><span class="pre">batched=True</span></code> to map our function to each batch in our dataset. Our final prepared dataset will only contain the following features: <code class="docutils literal notranslate"><span class="pre">['input_ids',</span> <span class="pre">'token_type_ids',</span> <span class="pre">'attention_mask',</span> <span class="pre">'labels']</span></code>, all other features are removed.</p>
<p>Now let’s wrap our datasets inside a pytorch dataloader as shown below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># to pad the inputs and labels in a batch to same size</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DataCollatorForTokenClassification</span>
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">collate_fn</span> <span class="o">=</span> <span class="n">DataCollatorForTokenClassification</span><span class="p">(</span><span class="n">tokenizer</span><span class="o">=</span><span class="n">tokenizer</span><span class="p">)</span>

<span class="c1"># training dataloader</span>
<span class="n">train_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">prepared_datasets</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">],</span> 
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> 
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
    <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_fn</span>
    <span class="p">)</span>

<span class="c1"># validation dataloader</span>
<span class="n">val_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">prepared_datasets</span><span class="p">[</span><span class="s1">&#39;validation&#39;</span><span class="p">],</span> 
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> 
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
    <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_fn</span>
    <span class="p">)</span>

<span class="c1"># test dataloader</span>
<span class="n">test_dl</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
    <span class="n">prepared_datasets</span><span class="p">[</span><span class="s1">&#39;test&#39;</span><span class="p">],</span> 
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> 
    <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
    <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_fn</span>
    <span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="training-the-model">
<h2>Training the model<a class="headerlink" href="#training-the-model" title="Permalink to this headline">¶</a></h2>
<p>As our dataloaders are in place, let’s discuss the steps to train our model. First we will create the model and optimizer for our training:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">optim</span>
<span class="c1"># token classification model</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForTokenClassification</span>

<span class="c1"># load the pretrained model</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForTokenClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span>
    <span class="n">checkpoint</span><span class="p">,</span> 
    <span class="n">num_labels</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
    <span class="p">)</span>
<span class="n">opt</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1.23e-4</span><span class="p">)</span>
</pre></div>
</div>
<p>The learning rate is obtained using the this <a class="reference external" href="https://github.com/davidtvs/pytorch-lr-finder">learning rate finder</a>. I made some hacky tweaks to make it work for this specific application.</p>
<p>We will obviously use a GPU for training the model, so we need to move our dataloaders, model and the optimizer to it. We will use huggingface’s accelerate library for this. If not already installed you can do it by running the command <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">accelerate</span></code> from your terminal.</p>
<p>With accelerate, moving everything to GPU is as simple as this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">accelerate</span> <span class="kn">import</span> <span class="n">Accelerator</span>

<span class="n">accelerator</span> <span class="o">=</span> <span class="n">Accelerator</span><span class="p">()</span>
<span class="n">train_dl</span><span class="p">,</span> <span class="n">val_dl</span><span class="p">,</span> <span class="n">test_dl</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">opt</span> <span class="o">=</span> <span class="n">accelerator</span><span class="o">.</span><span class="n">prepare</span><span class="p">(</span>
    <span class="n">train_dl</span><span class="p">,</span> 
    <span class="n">val_dl</span><span class="p">,</span> 
    <span class="n">test_dl</span><span class="p">,</span> 
    <span class="n">model</span><span class="p">,</span> 
    <span class="n">opt</span>
<span class="p">)</span>
</pre></div>
</div>
<p>We now need a metric to measure the performance of our model after each epoch. We will use the ‘seqeval’ framework for this, which gives the f1-score, precision, recall and overall accuracy. You can install it by running <code class="docutils literal notranslate"><span class="pre">pip</span> <span class="pre">install</span> <span class="pre">seqeval</span></code> on your terminal.</p>
<p>For calculating the metrics, we need to pass in the predictions and corresponding labels in string format to seqeval, just like shown below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_metric</span>

<span class="n">metric</span> <span class="o">=</span> <span class="n">load_metric</span><span class="p">(</span><span class="s1">&#39;seqeval&#39;</span><span class="p">)</span>

<span class="n">targets</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;O&#39;</span><span class="p">,</span> <span class="s1">&#39;B-PER&#39;</span><span class="p">,</span> <span class="s1">&#39;I-PER&#39;</span><span class="p">,</span> <span class="s1">&#39;O&#39;</span><span class="p">,</span> <span class="s1">&#39;O&#39;</span><span class="p">,</span> <span class="s1">&#39;O&#39;</span><span class="p">]</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;O&#39;</span><span class="p">,</span> <span class="s1">&#39;B-PER&#39;</span><span class="p">,</span> <span class="s1">&#39;O&#39;</span><span class="p">,</span> <span class="s1">&#39;O&#39;</span><span class="p">,</span> <span class="s1">&#39;O&#39;</span><span class="p">,</span> <span class="s1">&#39;O&#39;</span><span class="p">]</span>

<span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="p">[</span><span class="n">predictions</span><span class="p">],</span> <span class="n">references</span><span class="o">=</span><span class="p">[</span><span class="n">targets</span><span class="p">])</span>
</pre></div>
</div>
<p>Running the above code will return this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s1">&#39;PER&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;precision&#39;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span> <span class="s1">&#39;recall&#39;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span> <span class="s1">&#39;f1&#39;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span> <span class="s1">&#39;number&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">},</span>
 <span class="s1">&#39;overall_precision&#39;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
 <span class="s1">&#39;overall_recall&#39;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
 <span class="s1">&#39;overall_f1&#39;</span><span class="p">:</span> <span class="mf">0.0</span><span class="p">,</span>
 <span class="s1">&#39;overall_accuracy&#39;</span><span class="p">:</span> <span class="mf">0.8333333333333334</span><span class="p">}</span>
</pre></div>
</div>
<p>From the above outputs, we will take the <code class="docutils literal notranslate"><span class="pre">overall_accuracy</span></code> and print it after each epoch to measure the performance of our model on validation set.</p>
<p>Since seqeval require the predictions and labels to be in string format, lets write a function to convert them from numbers to string format.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">process_preds_and_labels</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">targets</span><span class="p">):</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">preds</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">preds</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="n">targets</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>

    <span class="n">true_targets</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">[</span><span class="n">labels</span><span class="p">[</span><span class="n">t</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span> <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">target</span> <span class="k">if</span> <span class="n">t</span><span class="o">!=-</span><span class="mi">100</span><span class="p">]</span> 
        <span class="k">for</span> <span class="n">target</span> <span class="ow">in</span> <span class="n">targets</span>
        <span class="p">]</span>
    <span class="n">true_preds</span> <span class="o">=</span> <span class="p">[</span>
        <span class="p">[</span><span class="n">labels</span><span class="p">[</span><span class="n">p</span><span class="o">.</span><span class="n">item</span><span class="p">()]</span> <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">pred</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span> <span class="k">if</span> <span class="n">t</span><span class="o">!=-</span><span class="mi">100</span><span class="p">]</span> 
        <span class="k">for</span> <span class="n">pred</span><span class="p">,</span> <span class="n">target</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
        <span class="p">]</span>

    <span class="k">return</span> <span class="n">true_preds</span><span class="p">,</span> <span class="n">true_targets</span>
</pre></div>
</div>
<p>Now let’s write a function for our training loop which takes a dataloader as input to get the prediction from the model, calculate the loss and finally do a backward pass and step the optimizer. For the backward pass we will use <code class="docutils literal notranslate"><span class="pre">accelerator.backward(model.loss)</span></code> instead of <code class="docutils literal notranslate"><span class="pre">model.loss.backward()</span></code> as we’ve used accelerate to move everything to GPU.</p>
<p>The model will return the predictions as well as the loss which can be accessed by <code class="docutils literal notranslate"><span class="pre">model.logits</span></code> and <code class="docutils literal notranslate"><span class="pre">model.loss</span></code> respectively.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">run_training_loop</span><span class="p">(</span><span class="n">train_dl</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">train_dl</span><span class="p">:</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">batch</span><span class="p">)</span>
        <span class="n">accelerator</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">loss</span><span class="p">)</span>
        <span class="n">opt</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        
        <span class="c1"># convert target labels and predictions to string format for computing accuracy</span>
        <span class="n">preds</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">process_preds_and_labels</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">logits</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">])</span>
        <span class="c1"># add the target labels and predictions of this batch to seqeval</span>
        <span class="n">metric</span><span class="o">.</span><span class="n">add_batch</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">preds</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
</pre></div>
</div>
<p>Similarly, let’s write an evaluation loop as well.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">run_evaluation_loop</span><span class="p">(</span><span class="n">test_dl</span><span class="p">):</span>
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
        <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">test_dl</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">batch</span><span class="p">)</span>
            
            <span class="c1"># convert target labels and predictions to string format for computing accuracy</span>
            <span class="n">preds</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">process_preds_and_labels</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">logits</span><span class="p">,</span> <span class="n">batch</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">])</span>
            <span class="c1"># add the target labels and predictions of this batch to seqeval</span>
            <span class="n">metric</span><span class="o">.</span><span class="n">add_batch</span><span class="p">(</span><span class="n">predictions</span><span class="o">=</span><span class="n">preds</span><span class="p">,</span> <span class="n">references</span><span class="o">=</span><span class="n">labels</span><span class="p">)</span>
</pre></div>
</div>
<p>Now let’s use these functions to train our model for 3 epochs and save the model after each epoch.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">3</span>

<span class="k">for</span> <span class="n">epoch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>
    <span class="n">run_training_loop</span><span class="p">(</span><span class="n">train_dl</span><span class="p">)</span>
    <span class="c1"># compute training accuracy</span>
    <span class="n">train_acc</span> <span class="o">=</span> <span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">()[</span><span class="s1">&#39;overall_accuracy&#39;</span><span class="p">]</span>
    
    <span class="n">run_evaluation_loop</span><span class="p">(</span><span class="n">val_dl</span><span class="p">)</span>
    <span class="c1"># compute validation accuracy</span>
    <span class="n">val_acc</span> <span class="o">=</span> <span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">()[</span><span class="s1">&#39;overall_accuracy&#39;</span><span class="p">]</span>
    
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;epoch: </span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2"> train_acc: </span><span class="si">{</span><span class="n">train_acc</span><span class="si">}</span><span class="s2"> val_acc: </span><span class="si">{</span><span class="n">val_acc</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    
    <span class="c1"># save the model at the end of epoch</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">state_dict</span><span class="p">(),</span> <span class="sa">f</span><span class="s2">&quot;model-v</span><span class="si">{</span><span class="n">epoch</span><span class="si">}</span><span class="s2">.pt&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>After training for 3 epochs, these are our final results:</p>
<p><img alt="fastprogress_ner" src="_images/fastprogress_ner.png" /></p>
<p>Inorder to get the nice looking table of results shown above, I made some modifications in the training code to use <a class="reference external" href="https://github.com/fastai/fastprogress">fastprogress</a>. It’s similar to the <a class="reference external" href="https://github.com/tqdm/tqdm">tqdm</a> progress bar but I love the nicely formatted output given by fastprogress which drags me to use it. Especially, the print messages become a mess while training for longer number of epochs on notebooks, but fastprogress solves that for me :)</p>
<p>We get a similar overall accuracy on the test set as well. The below code returns all the metrics(including overall accuracy) on the test set:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">run_evaluation_loop</span><span class="p">(</span><span class="n">test_dl</span><span class="p">)</span>
<span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="building-the-demo">
<h2>Building the demo<a class="headerlink" href="#building-the-demo" title="Permalink to this headline">¶</a></h2>
<p>And finally, we’ve completed the training and validation of the model. Now it’s time to build the gradio demo, so that everyone can use it without pressing <code class="docutils literal notranslate"><span class="pre">Shift+Enter</span></code> :)</p>
<p>As we have said in the begining of this chapter, we will build an application to convert a sentence from western context to Indian context.</p>
<p>For building our final application we need two more libraries - gradio and gensim(version <code class="docutils literal notranslate"><span class="pre">4.1.2</span></code>).</p>
<p>So let’s start building our demo. First let’s import gradio(to build our app) and gensim downloader to download the word2vec embeddings. Once the embeddings are downloaded, we could use the <code class="docutils literal notranslate"><span class="pre">.most_similar()</span></code> method of the api to find the most similar words for the named entities recognized by our model.</p>
<p>As an example, for an input sentence like this: <code class="docutils literal notranslate"><span class="pre">'My</span> <span class="pre">name</span> <span class="pre">is</span> <span class="pre">Sarah</span> <span class="pre">and</span> <span class="pre">I</span> <span class="pre">live</span> <span class="pre">in</span> <span class="pre">Columbia'</span></code>, we will get a modified sentence like this <code class="docutils literal notranslate"><span class="pre">'My</span> <span class="pre">name</span> <span class="pre">is</span> <span class="pre">Amanda</span> <span class="pre">and</span> <span class="pre">I</span> <span class="pre">live</span> <span class="pre">in</span> <span class="pre">Delhi</span> <span class="pre">'</span></code> where ‘Sarah’ and ‘Columbia’ are replaced with words that fits more into the Indian context.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">gradio</span> <span class="k">as</span> <span class="nn">gr</span>
<span class="kn">import</span> <span class="nn">gensim.downloader</span> <span class="k">as</span> <span class="nn">api</span>
</pre></div>
</div>
<p>Let’s download the word2vec embeddings.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">word2vec</span> <span class="o">=</span> <span class="n">api</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s1">&#39;word2vec-google-news-300&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>Now let’s write the function that takes in a sentence containing named entities familiar to western people and return the sentence modified to Indian context.</p>
<ol class="simple">
<li><p>The function will split the the input sentence into words, tokenize it and pass it to the model for making the predictions.</p></li>
<li><p>After that, we create a dictionary containing the words of the input sentence as keys and its corresponding string labels as values.</p></li>
<li><p>Then we find the word that is more related/closer to the word ‘India’ and farther away from the word ‘USA’ for all person, location and organisation entities in the sentence.</p></li>
<li><p>Then we replace these words with the one we got from step 3 and return the final text.</p></li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">prepare_output</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
    <span class="c1"># split the sentence into lis of words</span>
    <span class="n">text_split</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
    <span class="c1"># tokenize the words and return as pytorch tensors</span>
    <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span>
        <span class="n">text_split</span><span class="p">,</span> 
        <span class="n">is_split_into_words</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
        <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
        <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span>
        <span class="p">)</span>

    <span class="c1"># make predictions for each token</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">tokens</span><span class="p">)[</span><span class="s1">&#39;logits&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="c1"># find the named entity corresponding to each word</span>
    <span class="n">word_label</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">pred</span><span class="p">,</span> <span class="n">word_id</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">preds</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">tokens</span><span class="o">.</span><span class="n">word_ids</span><span class="p">()):</span>
        <span class="k">if</span> <span class="n">word_id</span><span class="o">!=</span><span class="kc">None</span><span class="p">:</span>
            <span class="n">label</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">pred</span><span class="p">]</span>
            <span class="n">word</span> <span class="o">=</span> <span class="n">text_split</span><span class="p">[</span><span class="n">word_id</span><span class="p">]</span>
            <span class="n">word_label</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">label</span>

    <span class="n">out_text</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
    <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">word_label</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>

        <span class="c1"># replace PER, LOC and ORG entities with words closer to &#39;India&#39;</span>
        <span class="k">if</span> <span class="n">label</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;-&#39;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;PER&#39;</span><span class="p">,</span> <span class="s1">&#39;LOC&#39;</span><span class="p">,</span> <span class="s1">&#39;ORG&#39;</span><span class="p">]:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">word</span> <span class="o">=</span> <span class="n">word2vec</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span>
                <span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;India&#39;</span><span class="p">,</span> <span class="n">word</span><span class="p">],</span> 
                <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;USA&#39;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">1</span>
                <span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
        <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
            <span class="k">pass</span>
        <span class="n">out_text</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2"> &quot;</span>
    <span class="k">return</span> <span class="n">out_text</span>
</pre></div>
</div>
<p>Let’s test this with an example,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">prepare_output</span><span class="p">(</span><span class="s2">&quot;My name is Mitchell and I live in Paris&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>And here’s the output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="s1">&#39;My name is Mukherjee and I live in Delhi&#39;</span>
</pre></div>
</div>
<p>‘Mitchell’ got replaced by ‘Mukherjee’ and ‘Paris’ got replaced by ‘Delhi’.</p>
<p>This function may not work as expected in some cases where the name of a location has two words like in ‘New York’. Our function will consider ‘New York’ as two unrelated words and replace both of them separately instead of considering them as a single entity like below:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">prepare_output</span><span class="p">(</span><span class="s2">&quot;My name is Mitchell and I live in New York&quot;</span><span class="p">)</span>
</pre></div>
</div>
<p>which returns,</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="s1">&#39;My name is Mukherjee and I live in Delhi Delhi&#39;</span>
</pre></div>
</div>
<p>We don’t want this to happen, so here is a slightly modified function to consider location or organisation names as a single word(join them by a ‘_’), which will convert ‘New York’ to ‘New_York’ and ‘San Francisco’ to ‘San_Francisco’.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">prepare_output</span><span class="p">(</span><span class="n">text</span><span class="p">):</span>
  <span class="n">text_split</span> <span class="o">=</span> <span class="n">text</span><span class="o">.</span><span class="n">split</span><span class="p">()</span>
  <span class="n">tokens</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text_split</span><span class="p">,</span> <span class="n">is_split_into_words</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;pt&#39;</span><span class="p">)</span>
  <span class="n">preds</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="o">**</span><span class="n">tokens</span><span class="p">)[</span><span class="s1">&#39;logits&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

  <span class="n">out</span> <span class="o">=</span> <span class="p">{}</span>
  <span class="n">last_b_tag</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
  <span class="k">for</span> <span class="n">p</span><span class="p">,</span> <span class="n">w_id</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">preds</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">tokens</span><span class="o">.</span><span class="n">word_ids</span><span class="p">()):</span>
    <span class="k">if</span> <span class="n">w_id</span><span class="o">!=</span><span class="kc">None</span><span class="p">:</span>
      <span class="n">label</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">p</span><span class="p">]</span>
      <span class="n">label_split</span> <span class="o">=</span> <span class="n">label</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;-&#39;</span><span class="p">)</span>
      <span class="n">word</span> <span class="o">=</span> <span class="n">text_split</span><span class="p">[</span><span class="n">w_id</span><span class="p">]</span>
      
      <span class="k">if</span> <span class="n">word</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">out</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">label_split</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;I&#39;</span> <span class="ow">and</span> <span class="n">label_split</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">==</span><span class="n">last_b_tag</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;-&#39;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]:</span>
          <span class="n">old_key</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">keys</span><span class="p">())[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
          <span class="n">new_key</span> <span class="o">=</span> <span class="n">old_key</span><span class="o">+</span><span class="sa">f</span><span class="s2">&quot; </span><span class="si">{</span><span class="n">word</span><span class="si">}</span><span class="s2">&quot;</span>
          <span class="n">out</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="n">old_key</span><span class="p">)</span>
          <span class="n">out</span><span class="p">[</span><span class="n">new_key</span><span class="p">]</span> <span class="o">=</span> <span class="n">last_b_tag</span>
        <span class="k">else</span><span class="p">:</span>
          <span class="n">out</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">label</span>
          
        <span class="k">if</span> <span class="p">(</span><span class="n">label_split</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">==</span><span class="s1">&#39;B&#39;</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="n">label_split</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;ORG&#39;</span><span class="p">,</span> <span class="s1">&#39;LOC&#39;</span><span class="p">]):</span>
          <span class="n">last_b_tag</span> <span class="o">=</span> <span class="n">label</span>

  <span class="n">out_text</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
  <span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">tag</span> <span class="ow">in</span> <span class="n">out</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">tag</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;-&#39;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="ow">in</span> <span class="p">[</span><span class="s1">&#39;PER&#39;</span><span class="p">,</span> <span class="s1">&#39;LOC&#39;</span><span class="p">,</span> <span class="s1">&#39;ORG&#39;</span><span class="p">]:</span>
      <span class="k">try</span><span class="p">:</span>
        <span class="n">word</span> <span class="o">=</span> <span class="n">word2vec</span><span class="o">.</span><span class="n">most_similar</span><span class="p">(</span><span class="n">positive</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;India&#39;</span><span class="p">,</span> <span class="n">word</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39; &#39;</span><span class="p">,</span> <span class="s1">&#39;_&#39;</span><span class="p">)],</span> <span class="n">negative</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;USA&#39;</span><span class="p">],</span> <span class="n">topn</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
      <span class="k">except</span> <span class="ne">KeyError</span><span class="p">:</span>
        <span class="k">pass</span>
    <span class="n">out_text</span> <span class="o">+=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">word</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s1">&#39;_&#39;</span><span class="p">,</span> <span class="s1">&#39; &#39;</span><span class="p">)</span><span class="si">}</span><span class="s2"> &quot;</span>
  <span class="k">return</span> <span class="n">out_text</span>
</pre></div>
</div>
<p>Now it’s time to launch our gradio demo, it’s as simple as passing the function to be executed(here it is <code class="docutils literal notranslate"><span class="pre">prepare_output</span></code>), the type of input(text box) and the type of output to be shown(text box) to <code class="docutils literal notranslate"><span class="pre">gr.Interface()</span></code> just like this:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">interface</span> <span class="o">=</span> <span class="n">gr</span><span class="o">.</span><span class="n">Interface</span><span class="p">(</span>
    <span class="n">prepare_output</span><span class="p">,</span>
    <span class="n">inputs</span><span class="o">=</span><span class="n">gr</span><span class="o">.</span><span class="n">inputs</span><span class="o">.</span><span class="n">Textbox</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;Input text&quot;</span><span class="p">,</span> <span class="n">lines</span><span class="o">=</span><span class="mi">3</span><span class="p">),</span>
    <span class="n">outputs</span><span class="o">=</span><span class="n">gr</span><span class="o">.</span><span class="n">outputs</span><span class="o">.</span><span class="n">Textbox</span><span class="p">(</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;Output text&quot;</span><span class="p">),</span>
<span class="p">)</span>

<span class="c1"># launch the demo</span>
<span class="n">interface</span><span class="o">.</span><span class="n">launch</span><span class="p">()</span>
</pre></div>
</div>
<p><img alt="ner_colab_demo" src="_images/ner_colab_demo.png" /></p>
<p>And voila, we have our demo up and running!!</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="about.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">About this book</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="masked_language_modeling.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Masked language modeling</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Bipin Krishnan P<br/>
        
            &copy; Copyright 2022.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>
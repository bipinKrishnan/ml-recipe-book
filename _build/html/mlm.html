
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Masked language modelling</title>
    
  <link href="_static/css/theme.css" rel="stylesheet">
  <link href="_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/togglebutton.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://github.com/mlm.html" />
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Image segmentation" href="cv.html" />
    <link rel="prev" title="Named entity recognition" href="ner.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/logo.png" class="logo" alt="logo">
      
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="index.html">
   About this book
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  NLP
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="ner.html">
   Named entity recognition
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Masked language modelling
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Computer vision
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="cv.html">
   Image segmentation
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="_sources/mlm.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/bipinkrishnan/ml-powered-apps"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        
        
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#what-is-masked-language-modelling">
   What is masked language modelling?
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#preparing-the-dataset">
   Preparing the dataset
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#downloading-the-dataset">
     Downloading the dataset
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#preprocessing-the-dataset">
     Preprocessing the dataset
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <section id="masked-language-modelling">
<h1>Masked language modelling<a class="headerlink" href="#masked-language-modelling" title="Permalink to this headline">¶</a></h1>
<p>As the title suggests, this chapter is all about masked language modelling. The term may feel a bit alien to some of you, we will dig a bit deeper in this section to clear all your doubts about masked language modelling(MLM).</p>
<section id="what-is-masked-language-modelling">
<h2>What is masked language modelling?<a class="headerlink" href="#what-is-masked-language-modelling" title="Permalink to this headline">¶</a></h2>
<p>As humans we adapt to a field like medicine by going through an extensive 5 year MBBS course and then we apply our skills, similarly we make our transformer model knowledgeable in a specific domain like medicine by pretraining it using <strong>Masked Langauage Modelling(MLM)</strong>, so that our model will perform better, say for example, on a classification task related to medical domain.</p>
<p>As you have an understanding of why masked language modelling is used, I will show you how it’s done.</p>
<p>This is how the final inference will look like:</p>
<img alt="Maked language modelling" class="bg-primary mb-1 align-center" src="_images/mlm_inference.png" />
<p>We input a sentence with some words replaced/masked with a special token <code class="docutils literal notranslate"><span class="pre">[MASK]</span></code>. The job of the model is to predict the correct word to fill in place of <code class="docutils literal notranslate"><span class="pre">[MASK]</span></code>. In the above figure the model predicts ‘happiness’ which makes input sentence <code class="docutils literal notranslate"><span class="pre">'The</span> <span class="pre">goal</span> <span class="pre">of</span> <span class="pre">life</span> <span class="pre">is</span> <span class="pre">[MASK].'</span></code> to <code class="docutils literal notranslate"><span class="pre">'The</span> <span class="pre">goal</span> <span class="pre">of</span> <span class="pre">life</span> <span class="pre">is</span> <span class="pre">happiness.'</span></code>.</p>
<p>The training data will have a randomly masked sentence as inputs to the model and the complete sentence(without any masks) as the label, just like shown below:</p>
<p>Input: <code class="docutils literal notranslate"><span class="pre">'The</span> <span class="pre">full</span> <span class="pre">cost</span> <span class="pre">of</span> <span class="pre">damage</span> <span class="pre">in</span> <span class="pre">[MASK]</span> <span class="pre">Stewart,</span> <span class="pre">one</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">areas</span> <span class="pre">[MASK]</span> <span class="pre">affected,</span> <span class="pre">is</span> <span class="pre">still</span> <span class="pre">being</span> <span class="pre">[MASK].'</span></code></p>
<p>Label: <code class="docutils literal notranslate"><span class="pre">'The</span> <span class="pre">full</span> <span class="pre">cost</span> <span class="pre">of</span> <span class="pre">damage</span> <span class="pre">in</span> <span class="pre">Newton</span> <span class="pre">Stewart,</span> <span class="pre">one</span> <span class="pre">of</span> <span class="pre">the</span> <span class="pre">areas</span> <span class="pre">worst</span> <span class="pre">affected,</span> <span class="pre">is</span> <span class="pre">still</span> <span class="pre">being</span> <span class="pre">assessed.'</span></code></p>
<p>The model will have to predict the correct words corresponding to the masks. In this way the model will learn about relationship between different words in a sentence. The task is some what similar to ‘fill in the blanks’ type questions that you might have encountered in your high school.</p>
<p>Now let’s look into the dataset that we will be using for this task.</p>
</section>
<section id="preparing-the-dataset">
<h2>Preparing the dataset<a class="headerlink" href="#preparing-the-dataset" title="Permalink to this headline">¶</a></h2>
<p>The dataset that we will be using is the <a class="reference external" href="https://huggingface.co/datasets/xsum">extreme summarization dataset</a> which is a collection of news articles and their corresponding summaries. We will drop the ‘summary’ feature and use only the new articles.</p>
<p>Now let’s look into the structure of our dataset.</p>
<section id="downloading-the-dataset">
<h3>Downloading the dataset<a class="headerlink" href="#downloading-the-dataset" title="Permalink to this headline">¶</a></h3>
<p>We will dounload the dataset from huggingface using their ‘datasets’ library.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span>

<span class="c1"># xsum -&gt; eXtreme SuMmarization</span>
<span class="n">raw_datasets</span> <span class="o">=</span> <span class="n">load_dataset</span><span class="p">(</span><span class="s1">&#39;xsum&#39;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="n">raw_datasets</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">DatasetDict</span><span class="p">({</span>
    <span class="n">train</span><span class="p">:</span> <span class="n">Dataset</span><span class="p">({</span>
        <span class="n">features</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;document&#39;</span><span class="p">,</span> <span class="s1">&#39;summary&#39;</span><span class="p">,</span> <span class="s1">&#39;id&#39;</span><span class="p">],</span>
        <span class="n">num_rows</span><span class="p">:</span> <span class="mi">204045</span>
    <span class="p">})</span>
    <span class="n">validation</span><span class="p">:</span> <span class="n">Dataset</span><span class="p">({</span>
        <span class="n">features</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;document&#39;</span><span class="p">,</span> <span class="s1">&#39;summary&#39;</span><span class="p">,</span> <span class="s1">&#39;id&#39;</span><span class="p">],</span>
        <span class="n">num_rows</span><span class="p">:</span> <span class="mi">11332</span>
    <span class="p">})</span>
    <span class="n">test</span><span class="p">:</span> <span class="n">Dataset</span><span class="p">({</span>
        <span class="n">features</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;document&#39;</span><span class="p">,</span> <span class="s1">&#39;summary&#39;</span><span class="p">,</span> <span class="s1">&#39;id&#39;</span><span class="p">],</span>
        <span class="n">num_rows</span><span class="p">:</span> <span class="mi">11334</span>
    <span class="p">})</span>
<span class="p">})</span>
</pre></div>
</div>
<p>As you can see from the above figure, we’ve a train, validation and test set. Of which, the training set is a huge one. So, for the sake of simplicity and for faster training, we will take a subset of the train set.</p>
<p>We will take 10,000 rows from the train set for training and 2000 rows for testing:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">train_size</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">test_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="mf">0.2</span><span class="o">*</span><span class="n">train_size</span><span class="p">)</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">42</span>

<span class="c1"># 10,000 rows for training and 2000 rows for testing</span>
<span class="n">downsampled_datasets</span> <span class="o">=</span> <span class="n">raw_datasets</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span>
    <span class="n">train_size</span><span class="o">=</span><span class="n">train_size</span><span class="p">,</span>
    <span class="n">test_size</span><span class="o">=</span><span class="n">test_size</span><span class="p">,</span>
    <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
<span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="n">downsampled_datasets</span><span class="p">)</span>
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">DatasetDict</span><span class="p">({</span>
    <span class="n">train</span><span class="p">:</span> <span class="n">Dataset</span><span class="p">({</span>
        <span class="n">features</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;document&#39;</span><span class="p">,</span> <span class="s1">&#39;summary&#39;</span><span class="p">,</span> <span class="s1">&#39;id&#39;</span><span class="p">],</span>
        <span class="n">num_rows</span><span class="p">:</span> <span class="mi">10000</span>
    <span class="p">})</span>
    <span class="n">test</span><span class="p">:</span> <span class="n">Dataset</span><span class="p">({</span>
        <span class="n">features</span><span class="p">:</span> <span class="p">[</span><span class="s1">&#39;document&#39;</span><span class="p">,</span> <span class="s1">&#39;summary&#39;</span><span class="p">,</span> <span class="s1">&#39;id&#39;</span><span class="p">],</span>
        <span class="n">num_rows</span><span class="p">:</span> <span class="mi">2000</span>
    <span class="p">})</span>
<span class="p">})</span>
</pre></div>
</div>
</section>
<section id="preprocessing-the-dataset">
<h3>Preprocessing the dataset<a class="headerlink" href="#preprocessing-the-dataset" title="Permalink to this headline">¶</a></h3>
<p>Now let’s prepare our dataset in a way that is needed for our model. Since our task is masked language modelling which is done to give domain knowledge to our model, we cannot afford to lose too much information from our input sentences due to truncation.</p>
<p>Since the maximum length of input sentence for the model we are using is 512, all the inputs that are longer than this will be truncated, which we don’t want to happen.</p>
<p>So, we will concatenate all of the sentences in a batch and divide them into smaller chunks, and then each chunk will be the inputs to the model as shown in the figure below:</p>
<img alt="_images/mlm_preprocess.png" class="align-center" src="_images/mlm_preprocess.png" />
<p>The only difference is that, instead of strings it will be tokens. So we will write a function to tokenize the input sentences and then do the above mentioned steps on the tokenized outputs.</p>
<p>Let’s load the tokenizer for our model:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoTokenizer</span>

<span class="c1"># model name</span>
<span class="n">checkpoint</span> <span class="o">=</span> <span class="s1">&#39;distilbert-base-uncased&#39;</span>
<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">checkpoint</span><span class="p">)</span>
</pre></div>
</div>
<p>We will be splitting our inputs into chunks of size 128. When splitting the inputs, the last chunk will be smaller than 128, so we will drop that for now.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">chunk_size</span> <span class="o">=</span> <span class="mi">128</span>

<span class="k">def</span> <span class="nf">create_chunks</span><span class="p">(</span><span class="n">examples</span><span class="p">):</span>
    <span class="c1"># tokenize the inputs</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">examples</span><span class="p">[</span><span class="s1">&#39;document&#39;</span><span class="p">])</span>
    <span class="c1"># cocatenate the inputs</span>
    <span class="n">concatenated_examples</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="nb">sum</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="p">[])</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">inputs</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">total_len</span> <span class="o">=</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">concatenated_examples</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">])</span><span class="o">//</span><span class="n">chunk_size</span><span class="p">)</span><span class="o">*</span><span class="n">chunk_size</span>
    
    <span class="c1"># create chunks of size 128</span>
    <span class="n">results</span> <span class="o">=</span> <span class="p">{</span>
        <span class="n">k</span><span class="p">:</span> <span class="p">[</span><span class="n">v</span><span class="p">[</span><span class="n">i</span><span class="p">:</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="n">chunk_size</span><span class="p">)]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">total_len</span><span class="p">,</span> <span class="n">chunk_size</span><span class="p">)]</span> 
        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">concatenated_examples</span><span class="o">.</span><span class="n">items</span><span class="p">()</span>
        <span class="p">}</span>
    
    <span class="n">results</span><span class="p">[</span><span class="s1">&#39;labels&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">results</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">results</span>
</pre></div>
</div>
<p>Let’s try out our function on a the dataset:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">preprocessed_datasets</span> <span class="o">=</span> <span class="n">downsampled_datasets</span><span class="o">.</span><span class="n">map</span><span class="p">(</span>
    <span class="n">create_chunks</span><span class="p">,</span> 
    <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
    <span class="n">remove_columns</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;document&#39;</span><span class="p">,</span> <span class="s1">&#39;summary&#39;</span><span class="p">,</span> <span class="s1">&#39;id&#39;</span><span class="p">]</span>
<span class="p">)</span>
</pre></div>
</div>
<p>Let’s check the size of our inputs now:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sample</span> <span class="o">=</span> <span class="n">preprocessed_datasets</span><span class="p">[</span><span class="s1">&#39;train&#39;</span><span class="p">][:</span><span class="mi">5</span><span class="p">]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">sample</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]:</span>
    <span class="n">input_length</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">input_length</span><span class="p">)</span> 
</pre></div>
</div>
<p>Output:</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="mi">128</span>
<span class="mi">128</span>
<span class="mi">128</span>
<span class="mi">128</span>
<span class="mi">128</span>
</pre></div>
</div>
<p>As you can see, the size of every input is now 128.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            <!-- Previous / next buttons -->
<div class='prev-next-area'> 
    <a class='left-prev' id="prev-link" href="ner.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Named entity recognition</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="cv.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Image segmentation</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Bipin Krishnan P<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>
{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from transformers import (\n    AutoFeatureExtractor, \n    AutoTokenizer, \n    VisionEncoderDecoderModel,\n    Seq2SeqTrainingArguments,\n    Seq2SeqTrainer, \n    default_data_collator,\n)\n\nfrom torch.utils.data import Dataset\n\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nfrom pathlib import Path\nfrom PIL import Image","metadata":{"execution":{"iopub.status.busy":"2022-04-03T06:23:47.284665Z","iopub.execute_input":"2022-04-03T06:23:47.285223Z","iopub.status.idle":"2022-04-03T06:23:53.741424Z","shell.execute_reply.started":"2022-04-03T06:23:47.285130Z","shell.execute_reply":"2022-04-03T06:23:53.740705Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(columns=['imgs'])\nimgs, captions = [], []\nroot_dir = Path(\"../input/flickr8k\")\n\nwith open(root_dir/\"captions.txt\", \"r\") as f:\n    content = f.readlines()\n    \nfor line in content:\n    line = line.strip().split(\"|\")\n    if line[1]=='1':\n        imgs.append(root_dir/\"images\"/line[0])\n        captions.append(line[-1])\n        \ndf.loc[:, 'imgs'] = imgs\ndf.loc[:, 'captions'] = captions","metadata":{"execution":{"iopub.status.busy":"2022-04-03T06:23:53.743082Z","iopub.execute_input":"2022-04-03T06:23:53.743426Z","iopub.status.idle":"2022-04-03T06:23:53.958403Z","shell.execute_reply.started":"2022-04-03T06:23:53.743395Z","shell.execute_reply":"2022-04-03T06:23:53.957610Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T06:23:53.959920Z","iopub.execute_input":"2022-04-03T06:23:53.960194Z","iopub.status.idle":"2022-04-03T06:23:53.974367Z","shell.execute_reply.started":"2022-04-03T06:23:53.960157Z","shell.execute_reply":"2022-04-03T06:23:53.973606Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"encoder_checkpoint = \"google/vit-base-patch16-224-in21k\"\ndecoder_checkpoint = \"gpt2\"\n\nfeature_extractor = AutoFeatureExtractor.from_pretrained(encoder_checkpoint)\ntokenizer = AutoTokenizer.from_pretrained(decoder_checkpoint)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"execution":{"iopub.status.busy":"2022-04-03T06:23:53.976067Z","iopub.execute_input":"2022-04-03T06:23:53.976368Z","iopub.status.idle":"2022-04-03T06:23:56.883786Z","shell.execute_reply.started":"2022-04-03T06:23:53.976332Z","shell.execute_reply":"2022-04-03T06:23:56.883069Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# maximum length for the captions\nmax_length = 128\nsample = df.iloc[0]\n\n# sample image\nimage = Image.open(sample['imgs']).convert('RGB')\n# sample caption\ncaption = sample['captions']\n\n# apply feature extractor on the sample image\ninputs = feature_extractor(images=image, return_tensors='pt')\n# apply tokenizer\noutputs = tokenizer(\n            caption, \n            max_length=max_length, \n            truncation=True, \n            padding='max_length',\n            return_tensors='pt',\n        )","metadata":{"execution":{"iopub.status.busy":"2022-04-03T06:23:56.885031Z","iopub.execute_input":"2022-04-03T06:23:56.885272Z","iopub.status.idle":"2022-04-03T06:23:56.938983Z","shell.execute_reply.started":"2022-04-03T06:23:56.885237Z","shell.execute_reply":"2022-04-03T06:23:56.938282Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"print(f\"Inputs:\\n{inputs}\\nOutputs:\\n{outputs}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class LoadDataset(Dataset):\n    def __init__(self, df):\n        self.images = df['imgs'].values\n        self.captions = df['captions'].values\n    \n    def __getitem__(self, idx):\n        # everything to return is stored inside this dict\n        inputs = dict()\n\n        # load the image and apply feature_extractor\n        image_path = str(self.images[idx])\n        image = Image.open(image_path).convert(\"RGB\")\n        image = feature_extractor(images=image, return_tensors='pt')\n\n        # load the caption and apply tokenizer\n        caption = self.captions[idx]\n        labels = tokenizer(\n            caption, \n            max_length=max_length, \n            truncation=True, \n            padding='max_length',\n            return_tensors='pt',\n        )['input_ids'][0]\n        \n        # store the inputs and labels in the dict we created\n        inputs['pixel_values'] = image['pixel_values'].squeeze()   \n        inputs['labels'] = labels\n        return inputs\n    \n    def __len__(self):\n        return len(self.images)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T06:23:56.940576Z","iopub.execute_input":"2022-04-03T06:23:56.940850Z","iopub.status.idle":"2022-04-03T06:23:56.949147Z","shell.execute_reply.started":"2022-04-03T06:23:56.940801Z","shell.execute_reply":"2022-04-03T06:23:56.948232Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_df, test_df = train_test_split(df, test_size=0.2, shuffle=True, random_state=42)\n\ntrain_ds = LoadDataset(train_df)\ntest_ds = LoadDataset(test_df)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T06:23:57.630880Z","iopub.execute_input":"2022-04-03T06:23:57.631327Z","iopub.status.idle":"2022-04-03T06:23:57.641332Z","shell.execute_reply.started":"2022-04-03T06:23:57.631290Z","shell.execute_reply":"2022-04-03T06:23:57.640610Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"next(iter(test_ds))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n    encoder_checkpoint, \n    decoder_checkpoint\n)\nmodel.config.decoder_start_token_id = tokenizer.bos_token_id\nmodel.config.pad_token_id = tokenizer.pad_token_id\n# model.config.vocab_size = model.config.decoder.vocab_size\nmodel.config.num_beams = 4","metadata":{"execution":{"iopub.status.busy":"2022-04-03T06:23:59.398458Z","iopub.execute_input":"2022-04-03T06:23:59.398730Z","iopub.status.idle":"2022-04-03T06:24:23.036386Z","shell.execute_reply.started":"2022-04-03T06:23:59.398694Z","shell.execute_reply":"2022-04-03T06:24:23.035632Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"batch = next(iter(train_ds))\n\nmodel(pixel_values=batch['pixel_values'].unsqueeze(0), labels=batch['labels'].unsqueeze(0))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_args = Seq2SeqTrainingArguments(\n    output_dir=\"image-caption-generator\", # name of the directory to store training outputs\n    evaluation_strategy=\"epoch\",          # evaluate after each epoch\n    per_device_train_batch_size=8,        # batch size during training\n    per_device_eval_batch_size=8,         # batch size during evaluation\n    learning_rate=5e-5,\n    weight_decay=0.01,                    # weight decay for AdamW optimizer\n    num_train_epochs=5,                   # number of epochs to train\n    save_strategy='epoch',                # save checkpoints after each epoch\n    report_to='none',                     # prevents logging to wandb, mlflow...\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model, \n    tokenizer=feature_extractor, \n    data_collator=default_data_collator,\n    train_dataset=train_ds,\n    eval_dataset=test_ds,\n    args=training_args,\n)","metadata":{"execution":{"iopub.status.busy":"2022-04-03T06:24:23.038127Z","iopub.execute_input":"2022-04-03T06:24:23.038574Z","iopub.status.idle":"2022-04-03T06:24:27.722382Z","shell.execute_reply.started":"2022-04-03T06:24:23.038535Z","shell.execute_reply":"2022-04-03T06:24:27.721653Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"execution":{"iopub.status.busy":"2022-04-03T06:24:27.723735Z","iopub.execute_input":"2022-04-03T06:24:27.724012Z","iopub.status.idle":"2022-04-03T07:01:30.584587Z","shell.execute_reply.started":"2022-04-03T06:24:27.723976Z","shell.execute_reply":"2022-04-03T07:01:30.583844Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"### Inference","metadata":{}},{"cell_type":"code","source":"import torch\nimport matplotlib.pyplot as plt","metadata":{"execution":{"iopub.status.busy":"2022-04-03T07:01:31.389653Z","iopub.execute_input":"2022-04-03T07:01:31.391108Z","iopub.status.idle":"2022-04-03T07:01:31.396449Z","shell.execute_reply.started":"2022-04-03T07:01:31.391066Z","shell.execute_reply":"2022-04-03T07:01:31.394168Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"inputs = test_ds[93]['pixel_values']\n\nmodel.eval()\nwith torch.no_grad():\n    # uncomment the below line if feature extractor is not applied to the image already\n    # inputs = feature_extractor(images=inputs, return_tensors='pt').pixel_values\n\n    # model prediction \n    out = model.generate(\n        inputs.unsqueeze(0).to('cuda'), # move inputs to GPU\n        num_beams=4, \n#         max_length=17\n        )\n\n# convert token ids to string format\ndecoded_out = tokenizer.decode(out[0], skip_special_tokens=True)\n\nprint(decoded_out)\nplt.axis('off')\nplt.imshow(torch.permute(inputs, (1, 2, 0)));","metadata":{"execution":{"iopub.status.busy":"2022-04-03T07:04:07.503415Z","iopub.execute_input":"2022-04-03T07:04:07.503664Z","iopub.status.idle":"2022-04-03T07:04:07.912817Z","shell.execute_reply.started":"2022-04-03T07:04:07.503635Z","shell.execute_reply":"2022-04-03T07:04:07.911307Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}